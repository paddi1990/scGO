{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RP11-34P13.7</th>\n",
       "      <th>FO538757.2</th>\n",
       "      <th>AP006222.2</th>\n",
       "      <th>RP4-669L17.10</th>\n",
       "      <th>RP5-857K21.4</th>\n",
       "      <th>RP11-206L10.9</th>\n",
       "      <th>LINC00115</th>\n",
       "      <th>FAM41C</th>\n",
       "      <th>RP11-54O7.1</th>\n",
       "      <th>RP11-54O7.3</th>\n",
       "      <th>...</th>\n",
       "      <th>AC145212.2</th>\n",
       "      <th>AC011043.1</th>\n",
       "      <th>AL592183.1</th>\n",
       "      <th>AC007325.4</th>\n",
       "      <th>AC007325.2</th>\n",
       "      <th>AL354822.1</th>\n",
       "      <th>AC004556.1</th>\n",
       "      <th>AC233755.2</th>\n",
       "      <th>AC233755.1</th>\n",
       "      <th>AC240274.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P1TLH_AAACCTGAGCAGCCTC_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1TLH_AAACCTGTCCTCATTA_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1TLH_AAACCTGTCTAAGCCA_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1TLH_AAACGGGAGTAGGCCA_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1TLH_AAACGGGGTTCGGGCT_1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20007 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          RP11-34P13.7  FO538757.2  AP006222.2  RP4-669L17.10  \\\n",
       "P1TLH_AAACCTGAGCAGCCTC_1           0.0         0.0     0.00000            0.0   \n",
       "P1TLH_AAACCTGTCCTCATTA_1           0.0         0.0     0.31476            0.0   \n",
       "P1TLH_AAACCTGTCTAAGCCA_1           0.0         0.0     0.00000            0.0   \n",
       "P1TLH_AAACGGGAGTAGGCCA_1           0.0         0.0     0.00000            0.0   \n",
       "P1TLH_AAACGGGGTTCGGGCT_1           0.0         0.0     0.00000            0.0   \n",
       "\n",
       "                          RP5-857K21.4  RP11-206L10.9  LINC00115  FAM41C  \\\n",
       "P1TLH_AAACCTGAGCAGCCTC_1           0.0            0.0        0.0     0.0   \n",
       "P1TLH_AAACCTGTCCTCATTA_1           0.0            0.0        0.0     0.0   \n",
       "P1TLH_AAACCTGTCTAAGCCA_1           0.0            0.0        0.0     0.0   \n",
       "P1TLH_AAACGGGAGTAGGCCA_1           0.0            0.0        0.0     0.0   \n",
       "P1TLH_AAACGGGGTTCGGGCT_1           0.0            0.0        0.0     0.0   \n",
       "\n",
       "                          RP11-54O7.1  RP11-54O7.3  ...  AC145212.2  \\\n",
       "P1TLH_AAACCTGAGCAGCCTC_1          0.0          0.0  ...         0.0   \n",
       "P1TLH_AAACCTGTCCTCATTA_1          0.0          0.0  ...         0.0   \n",
       "P1TLH_AAACCTGTCTAAGCCA_1          0.0          0.0  ...         0.0   \n",
       "P1TLH_AAACGGGAGTAGGCCA_1          0.0          0.0  ...         0.0   \n",
       "P1TLH_AAACGGGGTTCGGGCT_1          0.0          0.0  ...         0.0   \n",
       "\n",
       "                          AC011043.1  AL592183.1  AC007325.4  AC007325.2  \\\n",
       "P1TLH_AAACCTGAGCAGCCTC_1         0.0         0.0         0.0         0.0   \n",
       "P1TLH_AAACCTGTCCTCATTA_1         0.0         0.0         0.0         0.0   \n",
       "P1TLH_AAACCTGTCTAAGCCA_1         0.0         0.0         0.0         0.0   \n",
       "P1TLH_AAACGGGAGTAGGCCA_1         0.0         0.0         0.0         0.0   \n",
       "P1TLH_AAACGGGGTTCGGGCT_1         0.0         0.0         0.0         0.0   \n",
       "\n",
       "                          AL354822.1  AC004556.1  AC233755.2  AC233755.1  \\\n",
       "P1TLH_AAACCTGAGCAGCCTC_1         0.0         0.0         0.0         0.0   \n",
       "P1TLH_AAACCTGTCCTCATTA_1         0.0         0.0         0.0         0.0   \n",
       "P1TLH_AAACCTGTCTAAGCCA_1         0.0         0.0         0.0         0.0   \n",
       "P1TLH_AAACGGGAGTAGGCCA_1         0.0         0.0         0.0         0.0   \n",
       "P1TLH_AAACGGGGTTCGGGCT_1         0.0         0.0         0.0         0.0   \n",
       "\n",
       "                          AC240274.1  \n",
       "P1TLH_AAACCTGAGCAGCCTC_1         0.0  \n",
       "P1TLH_AAACCTGTCCTCATTA_1         0.0  \n",
       "P1TLH_AAACCTGTCTAAGCCA_1         0.0  \n",
       "P1TLH_AAACGGGAGTAGGCCA_1         0.0  \n",
       "P1TLH_AAACGGGGTTCGGGCT_1         0.0  \n",
       "\n",
       "[5 rows x 20007 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "data=pd.read_csv(\"data/macparland/GSE115469_Data.csv\",header=0,index_col=0,sep=\",\")\n",
    "\n",
    "data=data.T   #transpose to cell by gene\n",
    "\n",
    "annotation=pd.read_csv(\"data/macparland/GSE115469_CellClusterType.txt\",sep=\"\\t\")\n",
    "annotation[\"celltype\"]=annotation[\"CellType\"]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir=\"data/macparland\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20007\n",
      "5946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8444, 5946)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse=data\n",
    "\n",
    "\n",
    "#statistics of cells expressing each gene\n",
    "gene_expressed_cell_number=data_sparse.astype(bool).sum(axis=0)\n",
    "\n",
    "print(len(gene_expressed_cell_number))\n",
    "#filter gene expressed in less than 10 cells\n",
    "gene_expressed_cell_number=gene_expressed_cell_number[gene_expressed_cell_number>500]\n",
    "print(len(gene_expressed_cell_number))\n",
    "\n",
    "data_rm_sparse=data_sparse[gene_expressed_cell_number.index.tolist()]\n",
    "data_rm_sparse.shape           #10k cells, 4487 genes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "TF_gene_dict=pickle.load(open(\"human/TF_gene_dict\",\"rb\"))\n",
    "\n",
    "len(TF_gene_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate gene_to_TF_transform_matrix\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "TF_gene_dict=pickle.load(open(\"human/TF_gene_dict\",\"rb\"))\n",
    "\n",
    "\n",
    "gene_number=len(data_rm_sparse.columns.to_list())    \n",
    "\n",
    "TF_number=len(TF_gene_dict)\n",
    "\n",
    "gene_to_TF_transform_matrix=np.zeros((gene_number,TF_number))\n",
    "\n",
    "TF_list=TF_gene_dict.keys()\n",
    "for i,gene in enumerate(data_rm_sparse.columns):\n",
    "    try:\n",
    "        j=TF_list.index(\"gene\")\n",
    "        gene_to_TF_transform_matrix[i][j]=1\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "gene_to_TF_transform_matrix\n",
    "\n",
    "pickle.dump(gene_to_TF_transform_matrix,open(\"%s/gene_to_TF_transform_matrix\" %base_dir,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4787003\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#generate TF_mask\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gene_TF_dict=pickle.load(open(\"human/gene_TF_dict\",\"rb\"))\n",
    "\n",
    "gene_number = len(data_rm_sparse.columns.to_list())    #6033\n",
    "TF_number = len(TF_gene_dict)\n",
    "\n",
    "TF_mask = np.zeros((gene_number,TF_number))\n",
    "error_count=0\n",
    "\n",
    "for i,gene_id in enumerate(data_rm_sparse.columns):\n",
    "\n",
    "    for j,TF in enumerate(TF_gene_dict):\n",
    "        if TF in gene_TF_dict.get(gene_id,[]):\n",
    "            TF_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "        \n",
    "print(error_count)\n",
    "print(TF_mask)\n",
    "\n",
    "pickle.dump(TF_mask,open(\"%s/TF_mask\" %base_dir,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1946\n",
      "11494620\n"
     ]
    }
   ],
   "source": [
    "#generate GO_mask\n",
    "\n",
    "GO_dict={}\n",
    "with open(\"human/goa_human.gaf\") as f:\n",
    "    for line in f:\n",
    "        if line[0] == \"!\":\n",
    "            continue\n",
    "        \n",
    "        gene_id=line.split(\"\\t\")[2]\n",
    "        GO_term=line.split(\"\\t\")[4]\n",
    "        if GO_term not in GO_dict:\n",
    "            GO_dict[GO_term]=[]\n",
    "        GO_dict[GO_term].append(gene_id)\n",
    "\n",
    "\n",
    "GO_list=[]\n",
    "count=0\n",
    "for item in GO_dict:\n",
    "    if len(GO_dict[item])>=30:\n",
    "        count+=1\n",
    "        GO_list.append(item)\n",
    "print(count)\n",
    "\n",
    "\n",
    "gene_dict={}\n",
    "with open(\"human/goa_human.gaf\") as f:\n",
    "    for line in f:\n",
    "        if line[0]==\"!\":\n",
    "            continue\n",
    "        gene_id=line.split(\"\\t\")[2].upper()\n",
    "        GO_term=line.split(\"\\t\")[4]\n",
    "        if gene_id not in gene_dict:\n",
    "            gene_dict[gene_id]=[]\n",
    "        gene_dict[gene_id].append(GO_term)\n",
    "\n",
    "\n",
    "\n",
    "gene_number=len(gene_expressed_cell_number.index.tolist())    #6033\n",
    "GO_number=len(GO_list)  \n",
    "\n",
    "GO_mask=np.zeros((gene_number,GO_number))\n",
    "error_count=0\n",
    "\n",
    "for i,gene_id in enumerate(data_rm_sparse.columns):\n",
    "\n",
    "    for j,GO_term in enumerate(GO_list):\n",
    "        if GO_term in gene_dict.get(gene_id,\"GO:default\"):\n",
    "\n",
    "            GO_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "        \n",
    "print(error_count)\n",
    "\n",
    "pickle.dump(GO_mask,open(\"%s/GO_mask\" %base_dir,\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2332169\n"
     ]
    }
   ],
   "source": [
    "#generate GO_TF_mask\n",
    "\n",
    "TF_number=len(TF_gene_dict)\n",
    "GO_number=len(GO_list) \n",
    "\n",
    "GO_TF_mask=np.zeros((TF_number,GO_number))\n",
    "error_count=0\n",
    "\n",
    "for i,TF in enumerate(TF_gene_dict):\n",
    "    for j,GO in enumerate(GO_list):\n",
    "        if GO in gene_dict.get(TF,\"GO:default\"):\n",
    "            GO_TF_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "print(error_count)\n",
    "        \n",
    "GO_TF_mask\n",
    "\n",
    "pickle.dump(GO_TF_mask,open(\"%s/GO_TF_mask\" %base_dir,\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1-0 \taccuracy:\t 0.8904677323860273 \tloss:\t 0.9541652389859732\n",
      "fold 1-1 \taccuracy:\t 0.9034931912374186 \tloss:\t 0.14886066390086064\n",
      "fold 1-2 \taccuracy:\t 0.9123741859088218 \tloss:\t 0.04005538725780439\n",
      "fold 1-3 \taccuracy:\t 0.9230313795145056 \tloss:\t 0.010275682096526158\n",
      "fold 1-4 \taccuracy:\t 0.9248075784487862 \tloss:\t 0.0036640234860237193\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 450\u001b[0m\n\u001b[1;32m    441\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    443\u001b[0m \u001b[39m#reconstraction_input=reconstraction_model(outputs)\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[39m#reconstraction_loss = reconstraction_criterion(reconstraction_input, inputs)\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39m#combined_loss=loss+reconstraction_loss\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[39m#combined_loss.backward()\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    452\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    453\u001b[0m \u001b[39m#reconstraction_optimizer.step()\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \n\u001b[1;32m    455\u001b[0m \n\u001b[1;32m    456\u001b[0m \u001b[39m#reconstraction_running_loss += reconstraction_loss.item()\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sc/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/sc/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.conda/envs/sc/lib/python3.8/site-packages/torch/autograd/function.py:267\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    266\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 267\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "Cell \u001b[0;32mIn[9], line 120\u001b[0m, in \u001b[0;36mLinearFunction.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m    116\u001b[0m         grad_weight\u001b[39m=\u001b[39mgrad_weight\u001b[39m*\u001b[39mmask\n\u001b[1;32m    119\u001b[0m \u001b[39m#if bias is not None and ctx.need_input_grad[2]:\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[39mif\u001b[39;00m ctx\u001b[39m.\u001b[39;49mneeds_input_grad[\u001b[39m2\u001b[39m]:\n\u001b[1;32m    121\u001b[0m     grad_bias\u001b[39m=\u001b[39mgrad_output\u001b[39m.\u001b[39msum(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    123\u001b[0m \u001b[39mreturn\u001b[39;00m grad_input,grad_weight,grad_bias,grad_mask\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#GO_Net\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split as ts\n",
    "\n",
    "data_rm_sparse=data_rm_sparse\n",
    "\n",
    "data_rm_sparse.index=annotation[\"celltype\"].to_list()\n",
    "\n",
    "###############################################################\n",
    "gene_to_TF_transform_matrix=pickle.load(open(\"%s/gene_to_TF_transform_matrix\" %base_dir,\"rb\"))\n",
    "TF_mask=pickle.load(open(\"%s/TF_mask\" %base_dir,\"rb\"))\n",
    "GO_mask=pickle.load(open(\"%s/GO_mask\" %base_dir,\"rb\"))\n",
    "GO_TF_mask=pickle.load(open(\"%s/GO_TF_mask\" %base_dir,\"rb\"))\n",
    "###############################################################\n",
    "#data_annotation = pd.read_csv('data/macparland/GSE115469_CellClusterType.txt', sep=\"\\t\")\n",
    "#index_rename_dict = {key: value for key, value in zip(data_annotation['CellName'], data_annotation['CellType'])}\n",
    "#$data_rm_sparse=data_rm_sparse.rename(index=index_rename_dict)\n",
    "\n",
    "#normalize by row\n",
    "#data_rm_sparse = data_rm_sparse.apply(lambda row: row / np.linalg.norm(row), axis=1)\n",
    "\n",
    "#merge similar cell types\n",
    "#data_rm_sparse.index = data_rm_sparse.index.str.replace('Hepatocyte_\\d+', 'Hepatocyte', regex=True)\n",
    "#data_rm_sparse.index = data_rm_sparse.index.str.replace('gamma-delta_T_Cells_\\d+', 'gamma-delta_T_Cells', regex=True)\n",
    "\n",
    "\n",
    "#filter low count cells\n",
    "#data_rm_sparse = data_rm_sparse[data_rm_sparse.index != 'Hepatic_Stellate_Cells']\n",
    "\n",
    "\n",
    "#novel_cell_type = ['Plasma_Cells']\n",
    "\n",
    "#data_rm_sparse_novel = data_rm_sparse[data_rm_sparse.index.isin(novel_cell_type)]\n",
    "#data_rm_sparse_rest = data_rm_sparse[~data_rm_sparse.index.isin(novel_cell_type)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classes=[]\n",
    "for celltype in data_rm_sparse.index:\n",
    "    if celltype not in classes:\n",
    "        classes.append(celltype)\n",
    "#print(len(classes),classes)\n",
    "\n",
    "\n",
    "label_dict_revese={}\n",
    "label_dict={}\n",
    "for i,celltype in enumerate(classes):\n",
    "    label_dict[celltype]=i\n",
    "    label_dict_revese[i]=celltype\n",
    "label_dict\n",
    "################################################################\n",
    "\n",
    "\n",
    "\n",
    "def gen_mask(row,col,percent=0.5,num_zeros=None):\n",
    "    if num_zeros is None:\n",
    "        #Total number being masked is 0.5 by default\n",
    "        num_zeros=int((row*col)*percent)\n",
    "    \n",
    "    mask=np.hstack([np.zeros(num_zeros),np.ones(row*col-num_zeros)])\n",
    "    np.random.shuffle(mask)\n",
    "    return mask.reshape(row,col)\n",
    "\n",
    "class LinearFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    autograd function which masks it's weights by 'mask'.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Not that both forward and backword are @staticmethod\n",
    "\n",
    "    \n",
    "    #bias, mask is an optional argument\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias=None, mask=None):\n",
    "        if mask is not None:\n",
    "            #change weight to 0 where mask == 0\n",
    "\n",
    "            weight=weight*mask\n",
    " \n",
    "        output=input.mm(weight.t())\n",
    "\n",
    "        if bias is not None:\n",
    "            output+=bias.unsqueeze(0).expand_as(output)\n",
    "        \n",
    "        ctx.save_for_backward(input, weight, bias, mask)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    #This function has noly a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        input,weight,bias,mask = ctx.saved_tensors\n",
    "        grad_input=grad_weight=grad_bias=grad_mask=None\n",
    "        \n",
    "        #These meeds_input_grad checks are optional and there only to improve efficiency.\n",
    "        #If you want to make your code simpler, you can skip them. Returning gradients for\n",
    "        #inputs that don't require it is not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input=grad_output.mm(weight)\n",
    "        \n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight=grad_output.t().mm(input)\n",
    "            \n",
    "            if mask is not None:\n",
    "                \n",
    "                #change grad_weight to 0 where mask == 0\n",
    "                grad_weight=grad_weight*mask\n",
    "\n",
    "        \n",
    "        #if bias is not None and ctx.need_input_grad[2]:\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_bias=grad_output.sum(0).squeeze(0)\n",
    "        \n",
    "        return grad_input,grad_weight,grad_bias,grad_mask\n",
    "    \n",
    "\n",
    "       \n",
    "class CustomizedLinear(nn.Module):\n",
    "    def __init__(self,input_features,output_features, bias=None, mask=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        mask [numpy array]:\n",
    "            The shape is (n_input_fearues,n_output_features).\n",
    "            The elements are 0 or 1 which delcare un-connected or connected.\n",
    "            \n",
    "        bias [bool]:\n",
    "            flg of bias.\n",
    "        \"\"\"\n",
    "        super(CustomizedLinear,self).__init__()\n",
    "        self.input_features=input_features\n",
    "        self.out_features=output_features\n",
    "        \n",
    "        #nn.Parameter is a spetial kind of Tensor, that will get\n",
    "        #automatically registered as Module's parameter once it's assigned\n",
    "        #as an attribute\n",
    "        self.weight=nn.Parameter(torch.Tensor(self.out_features,self.input_features))\n",
    "        \n",
    "        if bias:\n",
    "\n",
    "            self.bias=nn.Parameter(torch.Tensor(self.out_features))\n",
    "        else:\n",
    "            #You should always register all possible parameters, but the\n",
    "            #optinal ones can be None if you want.\n",
    "            self.register_parameter(\"bias\",None)\n",
    "            \n",
    "        #Initialize the above parameters (weight and bias). Important!\n",
    "        self.init_params()\n",
    "        \n",
    "        #mask should be registered after weight and bias\n",
    "        if mask is not None:\n",
    "            mask=torch.tensor(mask,dtype=torch.float).t()\n",
    "            self.mask=nn.Parameter(mask,requires_grad=False)\n",
    "        else:\n",
    "            self.register_parameter(\"mask\",None)\n",
    "\n",
    "        \n",
    "    def init_params(self):\n",
    "        stdv=1./math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv,stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv,stdv)\n",
    "                \n",
    "    def forward(self,input):\n",
    "        #See the autograd section for explanation of what happens here.\n",
    "        \n",
    "        output=LinearFunction.apply(input,self.weight,self.bias,self.mask)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def extra_repr(self):\n",
    "        #(Optional) Set the extra informatioin about this module. You can test\n",
    "        #it by printing an object of this class.\n",
    "        return \"input_features={}, output_features={}, bias={}, mask={}\".format(\n",
    "            self.input_features, self.out_features,\n",
    "            self.bias is not None, self.mask is not None)\n",
    "        \n",
    "        \n",
    "class GO_Net(nn.Module):\n",
    "    def __init__(self,in_size,out_size,ratio=[0.006525,0,0]):\n",
    "        super(GO_Net,self).__init__()\n",
    "\n",
    "        self.gene_number=len(gene_expressed_cell_number.index.tolist())    #6033\n",
    "        self.TF_number=1209\n",
    "        self.GO_number=len(GO_list)\n",
    "        self.class_number=3\n",
    "\n",
    "        self.gene_to_TF_transform_matrix=torch.tensor(gene_to_TF_transform_matrix,dtype=torch.float32)\n",
    "    \n",
    "        \n",
    "        self.bn0=nn.BatchNorm1d(self.gene_number)\n",
    "        #self.fc1=CustomizedLinear(in_size,2290,mask=gen_mask(3443,2290,ratio[0]))  \n",
    "        #self.fc1=CustomizedLinear(in_size,1946,mask=gen_mask(2903,1946,ratio[0]))        \n",
    "        self.fc1=CustomizedLinear(in_size,self.GO_number,mask=GO_mask)    #GO_term\n",
    "        self.gene_to_GO_layer=CustomizedLinear(in_size,self.GO_number,mask=GO_mask)    #GO_term\n",
    "        #self.fc1=CustomizedLinear(in_size,2290,mask=np.ones((3443,2290)))\n",
    "    \n",
    "        self.bn1=nn.BatchNorm1d(self.GO_number)\n",
    "                \n",
    "        self.fc2=CustomizedLinear(self.GO_number,out_size,mask=gen_mask(self.GO_number,out_size,ratio[1]))\n",
    "        self.bn2=nn.BatchNorm1d(out_size)\n",
    "\n",
    "        self.gene_to_TF_layer=CustomizedLinear(self.gene_number,self.TF_number,mask=TF_mask)\n",
    "        self.TF_to_GO_layer=CustomizedLinear(self.TF_number,self.GO_number,mask=GO_TF_mask)\n",
    "        \n",
    "        self.fc3=CustomizedLinear(100,100,mask=gen_mask(100,100,ratio[1]))\n",
    "\n",
    "        self.fc4=CustomizedLinear(100,out_size,mask=gen_mask(100,out_size,ratio[1]))\n",
    "        \n",
    "        self.relu=nn.ReLU()\n",
    "        self.leaky_relu=nn.LeakyReLU()\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        for module in self.modules():\n",
    "            if isinstance(module,nn.Linear):\n",
    "                nn.init.uniform_(module.weight,a=0,b=1)\n",
    "            elif isinstance(module,(nn.BatchNorm1d,nn.GroupNorm)):\n",
    "                nn.init.constant_(module.weight,1)\n",
    "                nn.init.constant_(module.bias,0)\n",
    "\n",
    "                        \n",
    "    def forward(self,x):\n",
    "\n",
    "        #x=self.bn0(x)\n",
    "        TF_residul=torch.matmul(x,self.gene_to_TF_transform_matrix)\n",
    "\n",
    "        TF_derived_from_gene=self.gene_to_TF_layer(x)\n",
    "\n",
    "        TF_sum=TF_residul+TF_derived_from_gene\n",
    "        #TF_sum=TF_derived_from_gene\n",
    "\n",
    "        GO_derived_from_TF=self.TF_to_GO_layer(TF_sum)\n",
    "\n",
    "        GO_derived_from_gene=self.gene_to_GO_layer(x)\n",
    "\n",
    "        GO_sum=GO_derived_from_TF+GO_derived_from_gene\n",
    "\n",
    "        #x=self.bn0(x)\n",
    "        #x=self.fc1(x)\n",
    "        #x=self.bn1(x)\n",
    "        #x=self.relu(x)\n",
    "        #x=self.dropout(x)\n",
    "        GO_sum=self.leaky_relu(GO_sum)\n",
    "\n",
    "        #x=torch.tanh(x) \n",
    "        #print(161,self.fc1.weight)\n",
    "        x=self.fc2(GO_sum)\n",
    "        #x=self.bn2(x)\n",
    "        #x=self.relu(x)\n",
    "        #x=self.leaky_relu(x)\n",
    "        #x=self.fc3(x)\n",
    "        #x=self.leaky_relu(x)\n",
    "        #x=self.fc4(x)\n",
    " \n",
    "        return x,GO_sum,TF_derived_from_gene,GO_derived_from_TF\n",
    "\n",
    "\"\"\"\n",
    "class Reconstraction(nn.Module):\n",
    "    def __init__(self,in_size,out_size):\n",
    "        super(Reconstraction,self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_size, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 1000),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(1000, out_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\"\"\"     \n",
    "\n",
    "\n",
    " \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.x[index]\n",
    "        label = self.y[index]\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "def accuracy_score(y_test,y_pred):\n",
    "    t=0\n",
    "    f=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==y_pred[i]:\n",
    "            t+=1\n",
    "        else:\n",
    "            f+=1\n",
    "    return(t/(t+f))\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def make_weights_for_balanced_classes(dataset, nclasses):\n",
    "    count = [0] * nclasses\n",
    "    for item in dataset:\n",
    "        count[item[1]] += 1\n",
    "    weight_per_class = [0.] * nclasses\n",
    "    N = float(sum(count))\n",
    "    for i in range(nclasses):\n",
    "        weight_per_class[i] = N/float(count[i])\n",
    "    weight = [0] * len(dataset)\n",
    "    for idx, val in enumerate(dataset):\n",
    "        weight[idx] = weight_per_class[val[1]]\n",
    "    return weight\n",
    "\n",
    "\n",
    "class CustomWeightedRandomSampler(WeightedRandomSampler):\n",
    "    \"\"\"WeightedRandomSampler except allows for more than 2^24 samples to be sampled\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __iter__(self):\n",
    "        rand_tensor = np.random.choice(range(0, len(self.weights)),\n",
    "                                       size=self.num_samples,\n",
    "                                       p=self.weights.numpy() / torch.sum(self.weights).numpy(),\n",
    "                                       replace=self.replacement)\n",
    "        rand_tensor = torch.from_numpy(rand_tensor)\n",
    "        return iter(rand_tensor.tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#training\n",
    "input_size = len(data_rm_sparse.columns)\n",
    "output_size = len(classes)\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "#reconstraction_optimizer = optim.Adam(reconstraction_model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#reconstraction_criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "data_train_x=data_rm_sparse\n",
    "data_train_y=data_rm_sparse.index\n",
    "\n",
    "\n",
    "#5-fold cross validation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "# Generate 5-fold cross-validation indices\n",
    "kf = KFold(n_splits=num_folds, shuffle=False)\n",
    "fold_indices = list(kf.split(data))\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for fold, (train_indices, test_indices) in enumerate(fold_indices, start=1):\n",
    "\n",
    "    #define model and optimizer\n",
    "    model = GO_Net(input_size, output_size,ratio=[0,0,0])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    # Subset data and annotation based on indices\n",
    "    x_train = data_train_x.iloc[train_indices].to_numpy()\n",
    "    y_train = data_train_y[train_indices,]\n",
    "    \n",
    "    x_test = data_train_x.iloc[test_indices].to_numpy()\n",
    "    y_test = data_train_y[test_indices,]\n",
    "\n",
    "    # Continue with your operations on data_train, anno_train, data_test, and anno_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #x_train,x_test,y_train,y_test = ts(data_train_x.to_numpy(),data_train_y.to_numpy(),test_size=0.2,random_state=1, shuffle=True)\n",
    "\n",
    "    #x_train=x_train[0:400]\n",
    "    #y_train=y_train[0:400]\n",
    "\n",
    "    #label_dict={25:0,26:1,27:2,33:3,34:4}\n",
    "    y_train_relabeled=[label_dict[label] for label in y_train]\n",
    "    y_test_relabeled=[label_dict[label] for label in y_test]\n",
    "\n",
    "\n",
    "    #train_size=20000\n",
    "\n",
    "    #x_train=x_train[0:train_size]\n",
    "    #y_train_relabeled=y_train_relabeled[0:train_size]\n",
    "\n",
    "    train_data=MyDataset(x_train,y_train_relabeled)\n",
    "\n",
    "\n",
    "\n",
    "    #for unbalanced data\n",
    "    \"\"\"\n",
    "    weights=make_weights_for_balanced_classes(train_data,len(classes))\n",
    "    weights = torch.DoubleTensor(weights)\n",
    "    sampler = CustomWeightedRandomSampler(weights, len(weights))        #sampler for imbalanced classes\n",
    "    \"\"\"\n",
    "\n",
    "    #train_loader=DataLoader(train_data, batch_size=60, sampler=sampler)\n",
    "    train_loader=DataLoader(train_data, batch_size=60, shuffle=True)\n",
    "\n",
    "    num_epochs=15\n",
    "    # 训练模型\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        reconstraction_running_loss = 0.0\n",
    "\n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "            inputs, labels = batch\n",
    "            #print(labels)\n",
    "            inputs=Variable(inputs).to(torch.float32)\n",
    "            labels=Variable(labels).to(torch.long)\n",
    "            # 将梯度缓存清零\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 前向传播、计算损失和反向传播\n",
    "            outputs,_,_,_ = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            #reconstraction_input=reconstraction_model(outputs)\n",
    "            #reconstraction_loss = reconstraction_criterion(reconstraction_input, inputs)\n",
    "\n",
    "            #reconstraction_optimizer.zero_grad()\n",
    "\n",
    "            #combined_loss=loss+reconstraction_loss\n",
    "            #combined_loss.backward()\n",
    "            loss.backward()\n",
    "        \n",
    "            optimizer.step()\n",
    "            #reconstraction_optimizer.step()\n",
    "\n",
    "\n",
    "            #reconstraction_running_loss += reconstraction_loss.item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 40 == 0:\n",
    "                pass\n",
    "                #print(i)\n",
    "                #print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
    "            \n",
    "            if i>400:\n",
    "                break\n",
    "\n",
    "        test_data=MyDataset(x_test,y_test_relabeled)\n",
    "        test_loader=DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "        result=[]\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            inputs=Variable(inputs).to(torch.float32)\n",
    "\n",
    "            labels=Variable(labels).to(torch.long)\n",
    "            \n",
    "            outputs,_,_,_ = model(inputs)\n",
    "            pred = list(torch.max(outputs, 1)[1].numpy())\n",
    "            result.extend(pred)\n",
    "            #print(pred,labels)\n",
    "            if i>100:\n",
    "                break\n",
    "        accuracy = accuracy_score(y_test_relabeled[0:len(result)],result)\n",
    "\n",
    "                #########\n",
    "        print(\"fold %s-%s\" %(fold,epoch),\"\\taccuracy:\\t\",accuracy,\"\\tloss:\\t\",running_loss / len(train_loader))\n",
    "        \n",
    "        #save model\n",
    "        #pickle.dump(model,open(\"model/GO_heart.model\",\"wb\"))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hepatocyte_2\n",
      "GO:1900182 0.077733405\n",
      "GO:0060041 0.06973743\n",
      "GO:0007528 0.06663963\n",
      "GO:0120163 0.06584333\n",
      "GO:0048286 0.06518815\n",
      "GO:0046834 0.06464771\n",
      "GO:0030168 0.06260696\n",
      "GO:0030514 0.061650097\n",
      "GO:0035774 0.059527233\n",
      "GO:0007628 0.057913974\n",
      "GO:0043408 0.057329282\n",
      "GO:0007517 0.056171954\n",
      "GO:0030206 0.054399062\n",
      "GO:0060828 0.05364213\n",
      "GO:0060261 0.052877374\n",
      "GO:0006383 0.051947597\n",
      "GO:1905606 0.05174742\n",
      "GO:0040018 0.05145262\n",
      "GO:0051497 0.05110444\n",
      "GO:0046777 0.049910963\n",
      "GO:0051091 0.049896605\n",
      "GO:0042127 0.049639024\n",
      "GO:0043524 0.049281046\n",
      "GO:0000723 0.04873279\n",
      "GO:0035914 0.04813815\n",
      "GO:0070588 0.04805571\n",
      "GO:0006281 0.04804441\n",
      "GO:0001916 0.04786204\n",
      "GO:0021915 0.047420464\n",
      "GO:0099171 0.047342934\n",
      "GO:0032691 0.04677322\n",
      "GO:0016485 0.046621487\n",
      "GO:0007616 0.046363324\n",
      "GO:0031146 0.0462815\n",
      "GO:0030198 0.04566292\n",
      "GO:0045746 0.045563787\n",
      "GO:0006631 0.045457598\n",
      "GO:0008283 0.04512153\n",
      "GO:0090263 0.04510569\n",
      "GO:0002062 0.044879522\n",
      "GO:0051726 0.044817712\n",
      "GO:0006730 0.04476095\n",
      "GO:0019369 0.044508986\n",
      "GO:0045786 0.04390172\n",
      "GO:1900017 0.043777004\n",
      "GO:0051965 0.04333822\n",
      "GO:0051568 0.042665493\n",
      "GO:0006887 0.04256912\n",
      "GO:0034446 0.042554386\n",
      "GO:0051491 0.04238982\n",
      "GO:0035924 0.042326327\n",
      "GO:0000245 0.042186055\n",
      "GO:1905515 0.04217078\n",
      "GO:0042391 0.041900817\n",
      "GO:0050673 0.041899845\n",
      "GO:0043029 0.04134564\n",
      "GO:0016241 0.041292332\n",
      "GO:0031667 0.041168917\n",
      "GO:0045648 0.040985104\n",
      "GO:0002052 0.040909506\n",
      "GO:0051260 0.040907096\n",
      "GO:0090148 0.040572096\n",
      "GO:0006493 0.040539596\n",
      "GO:0034097 0.040457733\n",
      "GO:0006417 0.04032854\n",
      "GO:1904263 0.039508026\n",
      "GO:0048709 0.039294414\n",
      "GO:2001244 0.03921206\n",
      "GO:0070534 0.039148036\n",
      "GO:0071222 0.03903102\n",
      "GO:0007190 0.038955647\n"
     ]
    }
   ],
   "source": [
    "#gene contribution evaluation for each class weights\n",
    "GO_weights=model.fc2.weight.detach().numpy().T        #transpose. The weights in torch is transposed by default.\n",
    "gene_weights=model.fc1.weight.detach().numpy().T        #transpose. The weights in torch is transposed by default.\n",
    "\n",
    "masked_gene_weights=gene_weights*GO_mask\n",
    "\n",
    "top_genes_indices=[]\n",
    "\n",
    "for col in [16]:\n",
    "\n",
    "    celltype_4_weights=GO_weights[:,col]\n",
    "    print(classes[col])\n",
    "    #celltype_4_weights=[abs(weight) for weight in celltype_4_weights]\n",
    "    \n",
    "    \n",
    "    top_indices = np.argsort(celltype_4_weights)[::-1][:100]    #top 40 indices \n",
    "    \n",
    "\n",
    "    gene_count=0\n",
    "    for index in top_indices:\n",
    "\n",
    "        GO_term=GO_list[index]\n",
    "\n",
    "        if GO_type_dict[GO_term]==\"P\":\n",
    "            print(GO_term,celltype_4_weights[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5946, 1946)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO_type_dict={}\n",
    "with open(\"human/goa_human.gaf\") as f:\n",
    "    for line in f:\n",
    "        if line[0]==\"!\":\n",
    "            continue\n",
    "\n",
    "        GO_term=line.split(\"\\t\")[4]\n",
    "        GO_type=line.split(\"\\t\")[8]\n",
    "        if GO_term not in GO_type_dict:\n",
    "            GO_type_dict[GO_term]=GO_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Central_venous_LSECs',\n",
       " 'Cholangiocytes',\n",
       " 'Non-inflammatory_Macrophage',\n",
       " 'alpha-beta_T_Cells',\n",
       " 'Inflammatory_Macrophage',\n",
       " 'NK-like_Cells',\n",
       " 'gamma-delta_T_Cells_1',\n",
       " 'Hepatocyte_5',\n",
       " 'Portal_endothelial_Cells',\n",
       " 'gamma-delta_T_Cells_2',\n",
       " 'Periportal_LSECs',\n",
       " 'Hepatocyte_6',\n",
       " 'Mature_B_Cells',\n",
       " 'Hepatic_Stellate_Cells',\n",
       " 'Plasma_Cells',\n",
       " 'Erythroid_Cells',\n",
       " 'Hepatocyte_2',\n",
       " 'Hepatocyte_3',\n",
       " 'Hepatocyte_1',\n",
       " 'Hepatocyte_4']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
