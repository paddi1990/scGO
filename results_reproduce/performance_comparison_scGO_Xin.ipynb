{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2MP1</th>\n",
       "      <th>NAT1</th>\n",
       "      <th>NAT2</th>\n",
       "      <th>AACP</th>\n",
       "      <th>SERPINA3</th>\n",
       "      <th>AADAC</th>\n",
       "      <th>AAMP</th>\n",
       "      <th>AANAT</th>\n",
       "      <th>...</th>\n",
       "      <th>LOC101929766</th>\n",
       "      <th>LOC101929767</th>\n",
       "      <th>LOC101929768</th>\n",
       "      <th>LOC101929769</th>\n",
       "      <th>LOC101929770</th>\n",
       "      <th>LOC101929771</th>\n",
       "      <th>LOC101930100</th>\n",
       "      <th>LOC102723951</th>\n",
       "      <th>LOC102724004</th>\n",
       "      <th>LOC102724238</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sample_1</th>\n",
       "      <td>47.3396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.2047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.1983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2423</td>\n",
       "      <td>0.2423</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_2</th>\n",
       "      <td>24.0458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_3</th>\n",
       "      <td>2.2743</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.8214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.13</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>15.1863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample_5</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.9796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.6832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39851 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             A1BG  A2M  A2MP1      NAT1  NAT2  AACP  SERPINA3  AADAC  \\\n",
       "Sample_1  47.3396  0.0    0.0    0.0000   0.0   0.0    0.0000    0.0   \n",
       "Sample_2  24.0458  0.0    0.0    0.0000   0.0   0.0   14.0425    0.0   \n",
       "Sample_3   2.2743  0.0    0.0    0.0000   0.0   0.0    0.8412    0.0   \n",
       "Sample_4   0.0000  0.0    0.0    0.0000   0.0   0.0    0.0000    0.0   \n",
       "Sample_5   0.0000  0.0    0.0  130.9796   0.0   0.0    0.0000    0.0   \n",
       "\n",
       "              AAMP  AANAT  ...  LOC101929766  LOC101929767  LOC101929768  \\\n",
       "Sample_1   31.2047    0.0  ...           0.0        5.1983           0.0   \n",
       "Sample_2    0.0000    0.0  ...           0.0        0.1064           0.0   \n",
       "Sample_3    7.8214    0.0  ...           0.0        0.3188           0.0   \n",
       "Sample_4    0.0000    0.0  ...           0.0        0.0000           0.0   \n",
       "Sample_5  120.6832    0.0  ...           0.0        0.0000           0.0   \n",
       "\n",
       "          LOC101929769  LOC101929770  LOC101929771  LOC101930100  \\\n",
       "Sample_1           0.0           0.0           0.0          0.00   \n",
       "Sample_2           0.0           0.0           0.0          0.00   \n",
       "Sample_3           0.0           0.0           0.0          3.13   \n",
       "Sample_4           0.0           0.0           0.0          0.00   \n",
       "Sample_5           0.0           0.0           0.0          0.00   \n",
       "\n",
       "          LOC102723951  LOC102724004  LOC102724238  \n",
       "Sample_1        0.2423        0.2423        0.0000  \n",
       "Sample_2        0.0000        0.0000        0.0000  \n",
       "Sample_3        0.0000        0.0000       15.1863  \n",
       "Sample_4        0.0000        0.0000        0.0000  \n",
       "Sample_5        0.0000        0.0000        0.0000  \n",
       "\n",
       "[5 rows x 39851 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data\n",
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('data/xin/xin_data.csv',index_col=0,sep=',')\n",
    "\n",
    "data=data.T #transpose to cell by gene\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>celltype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beta</td>\n",
       "      <td>beta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beta</td>\n",
       "      <td>beta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beta</td>\n",
       "      <td>beta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beta</td>\n",
       "      <td>beta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>beta</td>\n",
       "      <td>beta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x celltype\n",
       "1  beta     beta\n",
       "2  beta     beta\n",
       "3  beta     beta\n",
       "4  beta     beta\n",
       "5  beta     beta"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation=pd.read_csv('data/xin/xin_celltype.csv',index_col=0,sep=',')\n",
    "annotation[\"celltype\"]=annotation[\"x\"]\n",
    "annotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir=\"data/xin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39851\n",
      "6958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1600, 6958)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse=data\n",
    "\n",
    "\n",
    "#statistics of cells expressing each gene\n",
    "gene_expressed_cell_number=data_sparse.astype(bool).sum(axis=0)\n",
    "\n",
    "print(len(gene_expressed_cell_number))\n",
    "#filter gene expressed in less than 10 cells\n",
    "gene_expressed_cell_number=gene_expressed_cell_number[gene_expressed_cell_number>500]\n",
    "print(len(gene_expressed_cell_number))\n",
    "\n",
    "data_rm_sparse=data_sparse[gene_expressed_cell_number.index.tolist()]\n",
    "data_rm_sparse.shape           #10k cells, 4487 genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "TF_gene_dict=pickle.load(open(\"human/TF_gene_dict\",\"rb\"))\n",
    "\n",
    "len(TF_gene_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate gene_to_TF_transform_matrix\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "TF_gene_dict=pickle.load(open(\"human/TF_gene_dict\",\"rb\"))\n",
    "\n",
    "\n",
    "gene_number=len(data_rm_sparse.columns.to_list())    \n",
    "\n",
    "TF_number=len(TF_gene_dict)\n",
    "\n",
    "gene_to_TF_transform_matrix=np.zeros((gene_number,TF_number))\n",
    "\n",
    "TF_list=TF_gene_dict.keys()\n",
    "for i,gene in enumerate(data_rm_sparse.columns):\n",
    "    try:\n",
    "        j=TF_list.index(\"gene\")\n",
    "        gene_to_TF_transform_matrix[i][j]=1\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "gene_to_TF_transform_matrix\n",
    "\n",
    "pickle.dump(gene_to_TF_transform_matrix,open(\"%s/gene_to_TF_transform_matrix\" %base_dir,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5959723\n",
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#generate TF_mask\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gene_TF_dict=pickle.load(open(\"human/gene_TF_dict\",\"rb\"))\n",
    "\n",
    "gene_number = len(data_rm_sparse.columns.to_list())    #6033\n",
    "TF_number = len(TF_gene_dict)\n",
    "\n",
    "TF_mask = np.zeros((gene_number,TF_number))\n",
    "error_count=0\n",
    "\n",
    "for i,gene_id in enumerate(data_rm_sparse.columns):\n",
    "\n",
    "    for j,TF in enumerate(TF_gene_dict):\n",
    "        if TF in gene_TF_dict.get(gene_id,[]):\n",
    "            TF_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "        \n",
    "print(error_count)\n",
    "print(TF_mask)\n",
    "\n",
    "pickle.dump(TF_mask,open(\"%s/TF_mask\" %base_dir,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1946\n",
      "13469481\n"
     ]
    }
   ],
   "source": [
    "#generate GO_mask\n",
    "\n",
    "GO_dict={}\n",
    "with open(\"human/goa_human.gaf\") as f:\n",
    "    for line in f:\n",
    "        if line[0] == \"!\":\n",
    "            continue\n",
    "        \n",
    "        gene_id=line.split(\"\\t\")[2]\n",
    "        GO_term=line.split(\"\\t\")[4]\n",
    "        if GO_term not in GO_dict:\n",
    "            GO_dict[GO_term]=[]\n",
    "        GO_dict[GO_term].append(gene_id)\n",
    "\n",
    "\n",
    "GO_list=[]\n",
    "count=0\n",
    "for item in GO_dict:\n",
    "    if len(GO_dict[item])>=30:\n",
    "        count+=1\n",
    "        GO_list.append(item)\n",
    "print(count)\n",
    "\n",
    "\n",
    "gene_dict={}\n",
    "with open(\"human/goa_human.gaf\") as f:\n",
    "    for line in f:\n",
    "        if line[0]==\"!\":\n",
    "            continue\n",
    "        gene_id=line.split(\"\\t\")[2].upper()\n",
    "        GO_term=line.split(\"\\t\")[4]\n",
    "        if gene_id not in gene_dict:\n",
    "            gene_dict[gene_id]=[]\n",
    "        gene_dict[gene_id].append(GO_term)\n",
    "\n",
    "\n",
    "\n",
    "gene_number=len(gene_expressed_cell_number.index.tolist())    #6033\n",
    "GO_number=len(GO_list)  \n",
    "\n",
    "GO_mask=np.zeros((gene_number,GO_number))\n",
    "error_count=0\n",
    "\n",
    "for i,gene_id in enumerate(data_rm_sparse.columns):\n",
    "\n",
    "    for j,GO_term in enumerate(GO_list):\n",
    "        if GO_term in gene_dict.get(gene_id,\"GO:default\"):\n",
    "\n",
    "            GO_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "        \n",
    "print(error_count)\n",
    "\n",
    "pickle.dump(GO_mask,open(\"%s/GO_mask\" %base_dir,\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2332169\n"
     ]
    }
   ],
   "source": [
    "#generate GO_TF_mask\n",
    "\n",
    "TF_number=len(TF_gene_dict)\n",
    "GO_number=len(GO_list) \n",
    "\n",
    "GO_TF_mask=np.zeros((TF_number,GO_number))\n",
    "error_count=0\n",
    "\n",
    "for i,TF in enumerate(TF_gene_dict):\n",
    "    for j,GO in enumerate(GO_list):\n",
    "        if GO in gene_dict.get(TF,\"GO:default\"):\n",
    "            GO_TF_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "print(error_count)\n",
    "        \n",
    "GO_TF_mask\n",
    "\n",
    "pickle.dump(GO_TF_mask,open(\"%s/GO_TF_mask\" %base_dir,\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_multiclass_f1_score(true_labels, predicted_labels):\n",
    "    if len(true_labels) != len(predicted_labels):\n",
    "        raise ValueError(\"Input lists must have the same length.\")\n",
    "\n",
    "    unique_labels = set(true_labels + predicted_labels)\n",
    "    f1_scores = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        true_positive = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == label and pred == label)\n",
    "        false_positive = sum(1 for true, pred in zip(true_labels, predicted_labels) if true != label and pred == label)\n",
    "        false_negative = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == label and pred != label)\n",
    "\n",
    "        precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "        recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    macro_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1-0  accuracy:\t 0.921875  loss:\t 7.662754362279719  \tf1 score:\t 0.5394527385389004\n",
      "fold 1-1  accuracy:\t 0.96875  loss:\t 1.8443830578841947  \tf1 score:\t 0.8317941668204285\n",
      "fold 1-2  accuracy:\t 0.928125  loss:\t 0.6343133213992629  \tf1 score:\t 0.6611723132875069\n",
      "fold 1-3  accuracy:\t 0.98125  loss:\t 0.15858792363233282  \tf1 score:\t 0.7389473149851565\n",
      "fold 1-4  accuracy:\t 0.971875  loss:\t 0.019054844048814603  \tf1 score:\t 0.7179046974239933\n",
      "fold 1-5  accuracy:\t 0.978125  loss:\t 0.0018618286651483503  \tf1 score:\t 0.8983743423758903\n",
      "fold 1-6  accuracy:\t 0.98125  loss:\t 7.608991392859078e-05  \tf1 score:\t 0.9106227106227106\n",
      "fold 1-7  accuracy:\t 0.98125  loss:\t 1.2527897867623237e-05  \tf1 score:\t 0.9106227106227106\n",
      "fold 1-8  accuracy:\t 0.98125  loss:\t 6.255712138162036e-06  \tf1 score:\t 0.9106227106227106\n",
      "fold 1-9  accuracy:\t 0.98125  loss:\t 4.534134126033994e-06  \tf1 score:\t 0.9106227106227106\n",
      "fold 1-10  accuracy:\t 0.98125  loss:\t 3.831029435504134e-06  \tf1 score:\t 0.9106227106227106\n",
      "fold 1-11  accuracy:\t 0.98125  loss:\t 3.135942120606346e-06  \tf1 score:\t 0.9106227106227106\n",
      "fold 1-12  accuracy:\t 0.98125  loss:\t 2.7547624531801516e-06  \tf1 score:\t 0.9106227106227106\n",
      "fold 1-13  accuracy:\t 0.98125  loss:\t 2.374836205102068e-06  \tf1 score:\t 0.9106227106227106\n",
      "fold 1-14  accuracy:\t 0.98125  loss:\t 2.1732490557853494e-06  \tf1 score:\t 0.9106227106227106\n",
      "fold 2-0  accuracy:\t 0.93125  loss:\t 7.580732601779428  \tf1 score:\t 0.5096484285032296\n",
      "fold 2-1  accuracy:\t 0.946875  loss:\t 2.4359093722940255  \tf1 score:\t 0.5707898700428032\n",
      "fold 2-2  accuracy:\t 0.95  loss:\t 0.6425517558236545  \tf1 score:\t 0.6985412266209368\n",
      "fold 2-3  accuracy:\t 0.95625  loss:\t 0.16457550085656653  \tf1 score:\t 0.6044510306402889\n",
      "fold 2-4  accuracy:\t 0.965625  loss:\t 0.006637236678360429  \tf1 score:\t 0.7155577181649789\n",
      "fold 2-5  accuracy:\t 0.959375  loss:\t 0.012058414146797626  \tf1 score:\t 0.6137715918661069\n",
      "fold 2-6  accuracy:\t 0.9625  loss:\t 0.0425135253947248  \tf1 score:\t 0.6014229011999732\n",
      "fold 2-7  accuracy:\t 0.9625  loss:\t 0.17642761661843356  \tf1 score:\t 0.7118405423946772\n",
      "fold 2-8  accuracy:\t 0.946875  loss:\t 0.009946291324687414  \tf1 score:\t 0.6850990581782719\n",
      "fold 2-9  accuracy:\t 0.96875  loss:\t 0.004235522965703963  \tf1 score:\t 0.7240876407543074\n",
      "fold 2-10  accuracy:\t 0.965625  loss:\t 2.133998101246219e-05  \tf1 score:\t 0.7190026979622739\n",
      "fold 2-11  accuracy:\t 0.965625  loss:\t 5.112283695283472e-06  \tf1 score:\t 0.7190026979622739\n",
      "fold 2-12  accuracy:\t 0.965625  loss:\t 3.33425794053387e-06  \tf1 score:\t 0.7190026979622739\n",
      "fold 2-13  accuracy:\t 0.965625  loss:\t 2.1119469251005474e-06  \tf1 score:\t 0.7190026979622739\n",
      "fold 2-14  accuracy:\t 0.965625  loss:\t 1.6402534750235211e-06  \tf1 score:\t 0.7190026979622739\n",
      "fold 3-0  accuracy:\t 0.93125  loss:\t 5.439257361672142  \tf1 score:\t 0.6449645091011315\n",
      "fold 3-1  accuracy:\t 0.925  loss:\t 1.0043964326618644  \tf1 score:\t 0.5625295508274232\n",
      "fold 3-2  accuracy:\t 0.890625  loss:\t 0.46739956169286057  \tf1 score:\t 0.633587786259542\n",
      "fold 3-3  accuracy:\t 0.96875  loss:\t 0.08618083054763699  \tf1 score:\t 0.7299422799422799\n",
      "fold 3-4  accuracy:\t 0.959375  loss:\t 0.07021743614855401  \tf1 score:\t 0.7012110523221634\n",
      "fold 3-5  accuracy:\t 0.975  loss:\t 0.005660584970224198  \tf1 score:\t 0.7406385281385282\n",
      "fold 3-6  accuracy:\t 0.975  loss:\t 0.00024025661144615057  \tf1 score:\t 0.8890697967941437\n",
      "fold 3-7  accuracy:\t 0.975  loss:\t 4.688500046663839e-05  \tf1 score:\t 0.8890697967941437\n",
      "fold 3-8  accuracy:\t 0.975  loss:\t 2.4268242227908456e-06  \tf1 score:\t 0.8890697967941437\n",
      "fold 3-9  accuracy:\t 0.975  loss:\t 1.6102197060971237e-06  \tf1 score:\t 0.8890697967941437\n",
      "fold 3-10  accuracy:\t 0.975  loss:\t 1.2460584421452643e-06  \tf1 score:\t 0.8890697967941437\n",
      "fold 3-11  accuracy:\t 0.975  loss:\t 9.013041831863365e-07  \tf1 score:\t 0.8890697967941437\n",
      "fold 3-12  accuracy:\t 0.975  loss:\t 7.497123768866616e-07  \tf1 score:\t 0.8890697967941437\n",
      "fold 3-13  accuracy:\t 0.975  loss:\t 6.5170352714361e-07  \tf1 score:\t 0.8890697967941437\n",
      "fold 3-14  accuracy:\t 0.975  loss:\t 5.862666821908714e-07  \tf1 score:\t 0.8890697967941437\n",
      "fold 4-0  accuracy:\t 0.978125  loss:\t 4.879119527272203  \tf1 score:\t 0.8996010936309444\n",
      "fold 4-1  accuracy:\t 0.959375  loss:\t 2.0579848473425955  \tf1 score:\t 0.7652237710931769\n",
      "fold 4-2  accuracy:\t 0.9875  loss:\t 0.33754890064236454  \tf1 score:\t 0.944046161916419\n",
      "fold 4-3  accuracy:\t 0.9875  loss:\t 0.17342354671084054  \tf1 score:\t 0.944046161916419\n",
      "fold 4-4  accuracy:\t 0.99375  loss:\t 0.053574705463747714  \tf1 score:\t 0.9733333333333334\n",
      "fold 4-5  accuracy:\t 0.990625  loss:\t 0.024272949858495364  \tf1 score:\t 0.9590426212722385\n",
      "fold 4-6  accuracy:\t 0.971875  loss:\t 0.017852353820476494  \tf1 score:\t 0.8562228545754081\n",
      "fold 4-7  accuracy:\t 0.971875  loss:\t 0.025173411021399552  \tf1 score:\t 0.900549052108153\n",
      "fold 4-8  accuracy:\t 0.978125  loss:\t 0.0006497367464129281  \tf1 score:\t 0.8942148760330578\n",
      "fold 4-9  accuracy:\t 0.978125  loss:\t 7.34154034805731e-05  \tf1 score:\t 0.8942148760330578\n",
      "fold 4-10  accuracy:\t 0.978125  loss:\t 7.618614197573754e-06  \tf1 score:\t 0.8942148760330578\n",
      "fold 4-11  accuracy:\t 0.978125  loss:\t 4.123814441763846e-06  \tf1 score:\t 0.8942148760330578\n",
      "fold 4-12  accuracy:\t 0.978125  loss:\t 2.9859705630095523e-06  \tf1 score:\t 0.8942148760330578\n",
      "fold 4-13  accuracy:\t 0.978125  loss:\t 2.311297015878854e-06  \tf1 score:\t 0.8942148760330578\n",
      "fold 4-14  accuracy:\t 0.978125  loss:\t 1.95923413335499e-06  \tf1 score:\t 0.8942148760330578\n",
      "fold 5-0  accuracy:\t 0.940625  loss:\t 5.129359388893301  \tf1 score:\t 0.6576816776497819\n",
      "fold 5-1  accuracy:\t 0.9375  loss:\t 0.9312003015596392  \tf1 score:\t 0.49400724209505625\n",
      "fold 5-2  accuracy:\t 0.85  loss:\t 0.21614401571922343  \tf1 score:\t 0.6997070272785096\n",
      "fold 5-3  accuracy:\t 0.953125  loss:\t 0.16176264850403432  \tf1 score:\t 0.7377165098249038\n",
      "fold 5-4  accuracy:\t 0.96875  loss:\t 0.05029776085514232  \tf1 score:\t 0.6892692560895325\n",
      "fold 5-5  accuracy:\t 0.965625  loss:\t 0.011988183759152247  \tf1 score:\t 0.6837073290854804\n",
      "fold 5-6  accuracy:\t 0.9625  loss:\t 0.04651750214593915  \tf1 score:\t 0.6360882814664327\n",
      "fold 5-7  accuracy:\t 0.96875  loss:\t 0.011676586174064741  \tf1 score:\t 0.6889453169900833\n",
      "fold 5-8  accuracy:\t 0.9625  loss:\t 0.002573017103974981  \tf1 score:\t 0.6819993490832525\n",
      "fold 5-9  accuracy:\t 0.965625  loss:\t 0.018609757418003244  \tf1 score:\t 0.6842996289424861\n",
      "fold 5-10  accuracy:\t 0.96875  loss:\t 0.002822881471422158  \tf1 score:\t 0.6889453169900833\n",
      "fold 5-11  accuracy:\t 0.965625  loss:\t 0.0013998225331365401  \tf1 score:\t 0.6842996289424861\n",
      "fold 5-12  accuracy:\t 0.953125  loss:\t 0.0001403129377859358  \tf1 score:\t 0.6641137170421982\n",
      "fold 5-13  accuracy:\t 0.9625  loss:\t 4.079829851632123e-06  \tf1 score:\t 0.6819993490832525\n",
      "fold 5-14  accuracy:\t 0.9625  loss:\t 6.732733630004423e-07  \tf1 score:\t 0.6819993490832525\n"
     ]
    }
   ],
   "source": [
    "#GO_Net\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split as ts\n",
    "\n",
    "data_rm_sparse=data_rm_sparse\n",
    "\n",
    "data_rm_sparse.index=annotation[\"celltype\"].to_list()\n",
    "\n",
    "###############################################################\n",
    "gene_to_TF_transform_matrix=pickle.load(open(\"%s/gene_to_TF_transform_matrix\" %base_dir,\"rb\"))\n",
    "TF_mask=pickle.load(open(\"%s/TF_mask\" %base_dir,\"rb\"))\n",
    "GO_mask=pickle.load(open(\"%s/GO_mask\" %base_dir,\"rb\"))\n",
    "GO_TF_mask=pickle.load(open(\"%s/GO_TF_mask\" %base_dir,\"rb\"))\n",
    "###############################################################\n",
    "#data_annotation = pd.read_csv('data/macparland/GSE115469_CellClusterType.txt', sep=\"\\t\")\n",
    "#index_rename_dict = {key: value for key, value in zip(data_annotation['CellName'], data_annotation['CellType'])}\n",
    "#$data_rm_sparse=data_rm_sparse.rename(index=index_rename_dict)\n",
    "\n",
    "#normalize by row\n",
    "#data_rm_sparse = data_rm_sparse.apply(lambda row: row / np.linalg.norm(row), axis=1)\n",
    "\n",
    "#merge similar cell types\n",
    "#data_rm_sparse.index = data_rm_sparse.index.str.replace('Hepatocyte_\\d+', 'Hepatocyte', regex=True)\n",
    "#data_rm_sparse.index = data_rm_sparse.index.str.replace('gamma-delta_T_Cells_\\d+', 'gamma-delta_T_Cells', regex=True)\n",
    "\n",
    "\n",
    "#filter low count cells\n",
    "#data_rm_sparse = data_rm_sparse[data_rm_sparse.index != 'Hepatic_Stellate_Cells']\n",
    "\n",
    "\n",
    "#novel_cell_type = ['Plasma_Cells']\n",
    "\n",
    "#data_rm_sparse_novel = data_rm_sparse[data_rm_sparse.index.isin(novel_cell_type)]\n",
    "#data_rm_sparse_rest = data_rm_sparse[~data_rm_sparse.index.isin(novel_cell_type)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classes=[]\n",
    "for celltype in data_rm_sparse.index:\n",
    "    if celltype not in classes:\n",
    "        classes.append(celltype)\n",
    "#print(len(classes),classes)\n",
    "\n",
    "\n",
    "label_dict_revese={}\n",
    "label_dict={}\n",
    "for i,celltype in enumerate(classes):\n",
    "    label_dict[celltype]=i\n",
    "    label_dict_revese[i]=celltype\n",
    "label_dict\n",
    "################################################################\n",
    "\n",
    "\n",
    "\n",
    "def gen_mask(row,col,percent=0.5,num_zeros=None):\n",
    "    if num_zeros is None:\n",
    "        #Total number being masked is 0.5 by default\n",
    "        num_zeros=int((row*col)*percent)\n",
    "    \n",
    "    mask=np.hstack([np.zeros(num_zeros),np.ones(row*col-num_zeros)])\n",
    "    np.random.shuffle(mask)\n",
    "    return mask.reshape(row,col)\n",
    "\n",
    "class LinearFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    autograd function which masks it's weights by 'mask'.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Not that both forward and backword are @staticmethod\n",
    "\n",
    "    \n",
    "    #bias, mask is an optional argument\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias=None, mask=None):\n",
    "        if mask is not None:\n",
    "            #change weight to 0 where mask == 0\n",
    "\n",
    "            weight=weight*mask\n",
    " \n",
    "        output=input.mm(weight.t())\n",
    "\n",
    "        if bias is not None:\n",
    "            output+=bias.unsqueeze(0).expand_as(output)\n",
    "        \n",
    "        ctx.save_for_backward(input, weight, bias, mask)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    #This function has noly a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        input,weight,bias,mask = ctx.saved_tensors\n",
    "        grad_input=grad_weight=grad_bias=grad_mask=None\n",
    "        \n",
    "        #These meeds_input_grad checks are optional and there only to improve efficiency.\n",
    "        #If you want to make your code simpler, you can skip them. Returning gradients for\n",
    "        #inputs that don't require it is not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input=grad_output.mm(weight)\n",
    "        \n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight=grad_output.t().mm(input)\n",
    "            \n",
    "            if mask is not None:\n",
    "                \n",
    "                #change grad_weight to 0 where mask == 0\n",
    "                grad_weight=grad_weight*mask\n",
    "\n",
    "        \n",
    "        #if bias is not None and ctx.need_input_grad[2]:\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_bias=grad_output.sum(0).squeeze(0)\n",
    "        \n",
    "        return grad_input,grad_weight,grad_bias,grad_mask\n",
    "    \n",
    "\n",
    "       \n",
    "class CustomizedLinear(nn.Module):\n",
    "    def __init__(self,input_features,output_features, bias=None, mask=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        mask [numpy array]:\n",
    "            The shape is (n_input_fearues,n_output_features).\n",
    "            The elements are 0 or 1 which delcare un-connected or connected.\n",
    "            \n",
    "        bias [bool]:\n",
    "            flg of bias.\n",
    "        \"\"\"\n",
    "        super(CustomizedLinear,self).__init__()\n",
    "        self.input_features=input_features\n",
    "        self.out_features=output_features\n",
    "        \n",
    "        #nn.Parameter is a spetial kind of Tensor, that will get\n",
    "        #automatically registered as Module's parameter once it's assigned\n",
    "        #as an attribute\n",
    "        self.weight=nn.Parameter(torch.Tensor(self.out_features,self.input_features))\n",
    "        \n",
    "        if bias:\n",
    "\n",
    "            self.bias=nn.Parameter(torch.Tensor(self.out_features))\n",
    "        else:\n",
    "            #You should always register all possible parameters, but the\n",
    "            #optinal ones can be None if you want.\n",
    "            self.register_parameter(\"bias\",None)\n",
    "            \n",
    "        #Initialize the above parameters (weight and bias). Important!\n",
    "        self.init_params()\n",
    "        \n",
    "        #mask should be registered after weight and bias\n",
    "        if mask is not None:\n",
    "            mask=torch.tensor(mask,dtype=torch.float).t()\n",
    "            self.mask=nn.Parameter(mask,requires_grad=False)\n",
    "        else:\n",
    "            self.register_parameter(\"mask\",None)\n",
    "\n",
    "        \n",
    "    def init_params(self):\n",
    "        stdv=1./math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv,stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv,stdv)\n",
    "                \n",
    "    def forward(self,input):\n",
    "        #See the autograd section for explanation of what happens here.\n",
    "        \n",
    "        output=LinearFunction.apply(input,self.weight,self.bias,self.mask)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def extra_repr(self):\n",
    "        #(Optional) Set the extra informatioin about this module. You can test\n",
    "        #it by printing an object of this class.\n",
    "        return \"input_features={}, output_features={}, bias={}, mask={}\".format(\n",
    "            self.input_features, self.out_features,\n",
    "            self.bias is not None, self.mask is not None)\n",
    "        \n",
    "        \n",
    "class GO_Net(nn.Module):\n",
    "    def __init__(self,in_size,out_size,ratio=[0.006525,0,0]):\n",
    "        super(GO_Net,self).__init__()\n",
    "\n",
    "        self.gene_number=len(gene_expressed_cell_number.index.tolist())    #6033\n",
    "        self.TF_number=1209\n",
    "        self.GO_number=len(GO_list)\n",
    "        self.class_number=3\n",
    "\n",
    "        self.gene_to_TF_transform_matrix=torch.tensor(gene_to_TF_transform_matrix,dtype=torch.float32)\n",
    "    \n",
    "        \n",
    "        self.bn0=nn.BatchNorm1d(self.gene_number)\n",
    "        #self.fc1=CustomizedLinear(in_size,2290,mask=gen_mask(3443,2290,ratio[0]))  \n",
    "        #self.fc1=CustomizedLinear(in_size,1946,mask=gen_mask(2903,1946,ratio[0]))        \n",
    "        self.fc1=CustomizedLinear(in_size,self.GO_number,mask=GO_mask)    #GO_term\n",
    "        self.gene_to_GO_layer=CustomizedLinear(in_size,self.GO_number,mask=GO_mask)    #GO_term\n",
    "        #self.fc1=CustomizedLinear(in_size,2290,mask=np.ones((3443,2290)))\n",
    "    \n",
    "        self.bn1=nn.BatchNorm1d(self.GO_number)\n",
    "                \n",
    "        self.fc2=CustomizedLinear(self.GO_number,out_size,mask=gen_mask(self.GO_number,out_size,ratio[1]))\n",
    "        self.bn2=nn.BatchNorm1d(out_size)\n",
    "\n",
    "        self.gene_to_TF_layer=CustomizedLinear(self.gene_number,self.TF_number,mask=TF_mask)\n",
    "        self.TF_to_GO_layer=CustomizedLinear(self.TF_number,self.GO_number,mask=GO_TF_mask)\n",
    "        \n",
    "        self.fc3=CustomizedLinear(100,100,mask=gen_mask(100,100,ratio[1]))\n",
    "\n",
    "        self.fc4=CustomizedLinear(100,out_size,mask=gen_mask(100,out_size,ratio[1]))\n",
    "        \n",
    "        self.relu=nn.ReLU()\n",
    "        self.leaky_relu=nn.LeakyReLU()\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        for module in self.modules():\n",
    "            if isinstance(module,nn.Linear):\n",
    "                nn.init.uniform_(module.weight,a=0,b=1)\n",
    "            elif isinstance(module,(nn.BatchNorm1d,nn.GroupNorm)):\n",
    "                nn.init.constant_(module.weight,1)\n",
    "                nn.init.constant_(module.bias,0)\n",
    "\n",
    "                        \n",
    "    def forward(self,x):\n",
    "\n",
    "        #x=self.bn0(x)\n",
    "        TF_residul=torch.matmul(x,self.gene_to_TF_transform_matrix)\n",
    "\n",
    "        TF_derived_from_gene=self.gene_to_TF_layer(x)\n",
    "\n",
    "        TF_sum=TF_residul+TF_derived_from_gene\n",
    "        #TF_sum=TF_derived_from_gene\n",
    "\n",
    "        GO_derived_from_TF=self.TF_to_GO_layer(TF_sum)\n",
    "\n",
    "        GO_derived_from_gene=self.gene_to_GO_layer(x)\n",
    "\n",
    "        GO_sum=GO_derived_from_TF+GO_derived_from_gene\n",
    "\n",
    "        #x=self.bn0(x)\n",
    "        #x=self.fc1(x)\n",
    "        #x=self.bn1(x)\n",
    "        #x=self.relu(x)\n",
    "        #x=self.dropout(x)\n",
    "        GO_sum=self.leaky_relu(GO_sum)\n",
    "\n",
    "        #x=torch.tanh(x) \n",
    "        #print(161,self.fc1.weight)\n",
    "        x=self.fc2(GO_sum)\n",
    "        #x=self.bn2(x)\n",
    "        #x=self.relu(x)\n",
    "        #x=self.leaky_relu(x)\n",
    "        #x=self.fc3(x)\n",
    "        #x=self.leaky_relu(x)\n",
    "        #x=self.fc4(x)\n",
    " \n",
    "        return x,GO_sum,TF_derived_from_gene,GO_derived_from_TF\n",
    "\n",
    "\"\"\"\n",
    "class Reconstraction(nn.Module):\n",
    "    def __init__(self,in_size,out_size):\n",
    "        super(Reconstraction,self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_size, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 1000),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(1000, out_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\"\"\"     \n",
    "\n",
    "\n",
    " \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.x[index]\n",
    "        label = self.y[index]\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "def accuracy_score(y_test,y_pred):\n",
    "    t=0\n",
    "    f=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==y_pred[i]:\n",
    "            t+=1\n",
    "        else:\n",
    "            f+=1\n",
    "    return(t/(t+f))\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def make_weights_for_balanced_classes(dataset, nclasses):\n",
    "    count = [0] * nclasses\n",
    "    for item in dataset:\n",
    "        count[item[1]] += 1\n",
    "    weight_per_class = [0.] * nclasses\n",
    "    N = float(sum(count))\n",
    "    for i in range(nclasses):\n",
    "        weight_per_class[i] = N/float(count[i])\n",
    "    weight = [0] * len(dataset)\n",
    "    for idx, val in enumerate(dataset):\n",
    "        weight[idx] = weight_per_class[val[1]]\n",
    "    return weight\n",
    "\n",
    "\n",
    "class CustomWeightedRandomSampler(WeightedRandomSampler):\n",
    "    \"\"\"WeightedRandomSampler except allows for more than 2^24 samples to be sampled\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __iter__(self):\n",
    "        rand_tensor = np.random.choice(range(0, len(self.weights)),\n",
    "                                       size=self.num_samples,\n",
    "                                       p=self.weights.numpy() / torch.sum(self.weights).numpy(),\n",
    "                                       replace=self.replacement)\n",
    "        rand_tensor = torch.from_numpy(rand_tensor)\n",
    "        return iter(rand_tensor.tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#training\n",
    "input_size = len(data_rm_sparse.columns)\n",
    "output_size = len(classes)\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "#reconstraction_optimizer = optim.Adam(reconstraction_model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#reconstraction_criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "data_train_x=data_rm_sparse\n",
    "data_train_y=data_rm_sparse.index\n",
    "\n",
    "\n",
    "#5-fold cross validation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "# Generate 5-fold cross-validation indices\n",
    "kf = KFold(n_splits=num_folds, shuffle=False)\n",
    "fold_indices = list(kf.split(data))\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for fold, (train_indices, test_indices) in enumerate(fold_indices, start=1):\n",
    "\n",
    "    #define model and optimizer\n",
    "    model = GO_Net(input_size, output_size,ratio=[0,0,0])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    # Subset data and annotation based on indices\n",
    "    x_train = data_train_x.iloc[train_indices].to_numpy()\n",
    "    y_train = data_train_y[train_indices,]\n",
    "    \n",
    "    x_test = data_train_x.iloc[test_indices].to_numpy()\n",
    "    y_test = data_train_y[test_indices,]\n",
    "\n",
    "    # Continue with your operations on data_train, anno_train, data_test, and anno_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #x_train,x_test,y_train,y_test = ts(data_train_x.to_numpy(),data_train_y.to_numpy(),test_size=0.2,random_state=1, shuffle=True)\n",
    "\n",
    "    #x_train=x_train[0:400]\n",
    "    #y_train=y_train[0:400]\n",
    "\n",
    "    #label_dict={25:0,26:1,27:2,33:3,34:4}\n",
    "    y_train_relabeled=[label_dict[label] for label in y_train]\n",
    "    y_test_relabeled=[label_dict[label] for label in y_test]\n",
    "\n",
    "\n",
    "    #train_size=20000\n",
    "\n",
    "    #x_train=x_train[0:train_size]\n",
    "    #y_train_relabeled=y_train_relabeled[0:train_size]\n",
    "\n",
    "    train_data=MyDataset(x_train,y_train_relabeled)\n",
    "\n",
    "\n",
    "\n",
    "    #for unbalanced data\n",
    "    \"\"\"\n",
    "    weights=make_weights_for_balanced_classes(train_data,len(classes))\n",
    "    weights = torch.DoubleTensor(weights)\n",
    "    sampler = CustomWeightedRandomSampler(weights, len(weights))        #sampler for imbalanced classes\n",
    "    \"\"\"\n",
    "\n",
    "    #train_loader=DataLoader(train_data, batch_size=60, sampler=sampler)\n",
    "    train_loader=DataLoader(train_data, batch_size=60, shuffle=True)\n",
    "\n",
    "    num_epochs=15\n",
    "    # 训练模型\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        reconstraction_running_loss = 0.0\n",
    "\n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "            inputs, labels = batch\n",
    "            #print(labels)\n",
    "            inputs=Variable(inputs).to(torch.float32)\n",
    "            labels=Variable(labels).to(torch.long)\n",
    "            # 将梯度缓存清零\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 前向传播、计算损失和反向传播\n",
    "            outputs,_,_,_ = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            #reconstraction_input=reconstraction_model(outputs)\n",
    "            #reconstraction_loss = reconstraction_criterion(reconstraction_input, inputs)\n",
    "\n",
    "            #reconstraction_optimizer.zero_grad()\n",
    "\n",
    "            #combined_loss=loss+reconstraction_loss\n",
    "            #combined_loss.backward()\n",
    "            loss.backward()\n",
    "        \n",
    "            optimizer.step()\n",
    "            #reconstraction_optimizer.step()\n",
    "\n",
    "\n",
    "            #reconstraction_running_loss += reconstraction_loss.item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 40 == 0:\n",
    "                pass\n",
    "                #print(i)\n",
    "                #print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
    "            \n",
    "            if i>400:\n",
    "                break\n",
    "\n",
    "        test_data=MyDataset(x_test,y_test_relabeled)\n",
    "        test_loader=DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "        result=[]\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            inputs=Variable(inputs).to(torch.float32)\n",
    "\n",
    "            labels=Variable(labels).to(torch.long)\n",
    "            \n",
    "            outputs,_,_,_ = model(inputs)\n",
    "            pred = list(torch.max(outputs, 1)[1].numpy())\n",
    "            result.extend(pred)\n",
    "            #print(pred,labels)\n",
    "            if i>100:\n",
    "                break\n",
    "        accuracy = accuracy_score(y_test_relabeled[0:len(result)],result)\n",
    "        f1_score = calculate_multiclass_f1_score(y_test_relabeled[0:len(result)],result)\n",
    "                #########\n",
    "        print(\"fold %s-%s\" %(fold,epoch),\"\\taccuracy:\\t\",accuracy,\"\\tloss:\\t\",running_loss / len(train_loader),\" \\tf1 score:\\t\",f1_score )\n",
    "        \n",
    "        #save model\n",
    "        #pickle.dump(model,open(\"model/GO_heart.model\",\"wb\"))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_relabeled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
