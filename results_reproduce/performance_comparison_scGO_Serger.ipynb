{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data and save to csv\n",
    "\n",
    "SegerMatrix   <- readRDS(url(\"https://storage.googleapis.com/cellid-cbl/SegerstolpeMatrix.rds\"))\n",
    "SegerMetaData <- readRDS(url(\"https://storage.googleapis.com/cellid-cbl/SegerstolpeMetaData2.rds\"))\n",
    "\n",
    "data=t(as.matrix(SegerMatrix))\n",
    "annotation=SegerMetaData\n",
    "\n",
    "annotation$celltype=annotation$cell.type\n",
    "\n",
    "write.table(data, file = \"data/segerstolpe/data.csv\", sep = \",\", row.names = TRUE, col.names = TRUE)\n",
    "write.table(annotation, file = \"data/segerstolpe/annotation.csv\", sep = \",\", row.names = TRUE, col.names = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data in python\n",
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('data/segerstolpe/data.csv', sep=',')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>donor</th>\n",
       "      <th>cell.type</th>\n",
       "      <th>celltype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HP1502401_H13</th>\n",
       "      <td>HP1502401</td>\n",
       "      <td>Gamma (PP) cells</td>\n",
       "      <td>Gamma (PP) cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HP1502401_J14</th>\n",
       "      <td>HP1502401</td>\n",
       "      <td>Alpha cells</td>\n",
       "      <td>Alpha cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HP1502401_B14</th>\n",
       "      <td>HP1502401</td>\n",
       "      <td>Beta cells</td>\n",
       "      <td>Beta cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HP1502401_A14</th>\n",
       "      <td>HP1502401</td>\n",
       "      <td>Acinar cells</td>\n",
       "      <td>Acinar cells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HP1502401_C14</th>\n",
       "      <td>HP1502401</td>\n",
       "      <td>Alpha cells</td>\n",
       "      <td>Alpha cells</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   donor         cell.type          celltype\n",
       "HP1502401_H13  HP1502401  Gamma (PP) cells  Gamma (PP) cells\n",
       "HP1502401_J14  HP1502401       Alpha cells       Alpha cells\n",
       "HP1502401_B14  HP1502401        Beta cells        Beta cells\n",
       "HP1502401_A14  HP1502401      Acinar cells      Acinar cells\n",
       "HP1502401_C14  HP1502401       Alpha cells       Alpha cells"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation=pd.read_csv(\"data/segerstolpe/annotation.csv\", sep = \",\")\n",
    "annotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir=\"data/segerstolpe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25454\n",
      "6253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2168, 6253)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse=data\n",
    "\n",
    "\n",
    "#statistics of cells expressing each gene\n",
    "gene_expressed_cell_number=data_sparse.astype(bool).sum(axis=0)\n",
    "\n",
    "print(len(gene_expressed_cell_number))\n",
    "#filter gene expressed in less than 10 cells\n",
    "gene_expressed_cell_number=gene_expressed_cell_number[gene_expressed_cell_number>1000]\n",
    "print(len(gene_expressed_cell_number))\n",
    "\n",
    "data_rm_sparse=data_sparse[gene_expressed_cell_number.index.tolist()]\n",
    "data_rm_sparse.shape           #10k cells, 4487 genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "TF_gene_dict=pickle.load(open(\"human/TF_gene_dict\",\"rb\"))\n",
    "\n",
    "len(TF_gene_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate gene_to_TF_transform_matrix\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "TF_gene_dict=pickle.load(open(\"human/TF_gene_dict\",\"rb\"))\n",
    "\n",
    "\n",
    "gene_number=len(data_rm_sparse.columns.to_list())    \n",
    "\n",
    "TF_number=len(TF_gene_dict)\n",
    "\n",
    "gene_to_TF_transform_matrix=np.zeros((gene_number,TF_number))\n",
    "\n",
    "TF_list=TF_gene_dict.keys()\n",
    "for i,gene in enumerate(data_rm_sparse.columns):\n",
    "    try:\n",
    "        j=TF_list.index(\"gene\")\n",
    "        gene_to_TF_transform_matrix[i][j]=1\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "gene_to_TF_transform_matrix\n",
    "\n",
    "pickle.dump(gene_to_TF_transform_matrix,open(\"%s/gene_to_TF_transform_matrix\" %base_dir,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4927624\n",
      "[[1. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#generate TF_mask\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "gene_TF_dict=pickle.load(open(\"human/gene_TF_dict\",\"rb\"))\n",
    "\n",
    "gene_number = len(data_rm_sparse.columns.to_list())    #6033\n",
    "TF_number = len(TF_gene_dict)\n",
    "\n",
    "TF_mask = np.zeros((gene_number,TF_number))\n",
    "error_count=0\n",
    "\n",
    "for i,gene_id in enumerate(data_rm_sparse.columns):\n",
    "\n",
    "    for j,TF in enumerate(TF_gene_dict):\n",
    "        if TF in gene_TF_dict.get(gene_id,[]):\n",
    "            TF_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "        \n",
    "print(error_count)\n",
    "print(TF_mask)\n",
    "\n",
    "pickle.dump(TF_mask,open(\"%s/TF_mask\" %base_dir,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1946\n",
      "12092814\n"
     ]
    }
   ],
   "source": [
    "#generate GO_mask\n",
    "\n",
    "GO_dict={}\n",
    "with open(\"human/goa_human.gaf\") as f:\n",
    "    for line in f:\n",
    "        if line[0] == \"!\":\n",
    "            continue\n",
    "        \n",
    "        gene_id=line.split(\"\\t\")[2]\n",
    "        GO_term=line.split(\"\\t\")[4]\n",
    "        if GO_term not in GO_dict:\n",
    "            GO_dict[GO_term]=[]\n",
    "        GO_dict[GO_term].append(gene_id)\n",
    "\n",
    "\n",
    "GO_list=[]\n",
    "count=0\n",
    "for item in GO_dict:\n",
    "    if len(GO_dict[item])>=30:\n",
    "        count+=1\n",
    "        GO_list.append(item)\n",
    "print(count)\n",
    "\n",
    "\n",
    "gene_dict={}\n",
    "with open(\"human/goa_human.gaf\") as f:\n",
    "    for line in f:\n",
    "        if line[0]==\"!\":\n",
    "            continue\n",
    "        gene_id=line.split(\"\\t\")[2].upper()\n",
    "        GO_term=line.split(\"\\t\")[4]\n",
    "        if gene_id not in gene_dict:\n",
    "            gene_dict[gene_id]=[]\n",
    "        gene_dict[gene_id].append(GO_term)\n",
    "\n",
    "\n",
    "\n",
    "gene_number=len(gene_expressed_cell_number.index.tolist())    #6033\n",
    "GO_number=len(GO_list)  \n",
    "\n",
    "GO_mask=np.zeros((gene_number,GO_number))\n",
    "error_count=0\n",
    "\n",
    "for i,gene_id in enumerate(data_rm_sparse.columns):\n",
    "\n",
    "    for j,GO_term in enumerate(GO_list):\n",
    "        if GO_term in gene_dict.get(gene_id,\"GO:default\"):\n",
    "\n",
    "            GO_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "        \n",
    "print(error_count)\n",
    "\n",
    "pickle.dump(GO_mask,open(\"%s/GO_mask\" %base_dir,\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2332169\n"
     ]
    }
   ],
   "source": [
    "#generate GO_TF_mask\n",
    "\n",
    "TF_number=len(TF_gene_dict)\n",
    "GO_number=len(GO_list) \n",
    "\n",
    "GO_TF_mask=np.zeros((TF_number,GO_number))\n",
    "error_count=0\n",
    "\n",
    "for i,TF in enumerate(TF_gene_dict):\n",
    "    for j,GO in enumerate(GO_list):\n",
    "        if GO in gene_dict.get(TF,\"GO:default\"):\n",
    "            GO_TF_mask[i][j]=1\n",
    "        else:\n",
    "            error_count+=1\n",
    "print(error_count)\n",
    "        \n",
    "GO_TF_mask\n",
    "\n",
    "pickle.dump(GO_TF_mask,open(\"%s/GO_TF_mask\" %base_dir,\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_multiclass_f1_score(true_labels, predicted_labels):\n",
    "    if len(true_labels) != len(predicted_labels):\n",
    "        raise ValueError(\"Input lists must have the same length.\")\n",
    "\n",
    "    unique_labels = set(true_labels + predicted_labels)\n",
    "    f1_scores = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        true_positive = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == label and pred == label)\n",
    "        false_positive = sum(1 for true, pred in zip(true_labels, predicted_labels) if true != label and pred == label)\n",
    "        false_negative = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == label and pred != label)\n",
    "\n",
    "        precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "        recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    macro_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1-0 \taccuracy:\t 0.9585253456221198 \tloss:\t 4.367747649670868  \tf1 score:\t 0.6100093361323679\n",
      "fold 1-1 \taccuracy:\t 0.9723502304147466 \tloss:\t 0.5303133957023765  \tf1 score:\t 0.6834171244080723\n",
      "fold 1-2 \taccuracy:\t 0.9907834101382489 \tloss:\t 0.6073217550356589  \tf1 score:\t 0.9216187050434008\n",
      "fold 1-3 \taccuracy:\t 0.9746543778801844 \tloss:\t 0.07003904608292691  \tf1 score:\t 0.8125726629531195\n",
      "fold 1-4 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.04055051020466259  \tf1 score:\t 0.9824236895576071\n",
      "fold 1-5 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.017199343132545505  \tf1 score:\t 0.9824236895576071\n",
      "fold 1-6 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.006889018457349235  \tf1 score:\t 0.9824236895576071\n",
      "fold 1-7 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.004712860794911492  \tf1 score:\t 0.9824236895576071\n",
      "fold 1-8 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.004320634527614637  \tf1 score:\t 0.9824236895576071\n",
      "fold 1-9 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.003999347014283118  \tf1 score:\t 0.9824236895576071\n",
      "fold 1-10 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.003718219722659028  \tf1 score:\t 0.9824236895576071\n",
      "fold 1-11 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.0034380407370039833  \tf1 score:\t 0.9824236895576071\n",
      "fold 1-12 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.003235151923867118  \tf1 score:\t 0.9824236895576071\n",
      "fold 1-13 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.0029814730093229284  \tf1 score:\t 0.9824236895576071\n",
      "fold 1-14 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.00275498474207262  \tf1 score:\t 0.9824236895576071\n",
      "fold 2-0 \taccuracy:\t 0.9493087557603687 \tloss:\t 3.2886492178286377  \tf1 score:\t 0.8242990434986364\n",
      "fold 2-1 \taccuracy:\t 0.9262672811059908 \tloss:\t 0.2688291318500115  \tf1 score:\t 0.8000790809583901\n",
      "fold 2-2 \taccuracy:\t 0.9700460829493087 \tloss:\t 0.04098538319434236  \tf1 score:\t 0.9175250666005528\n",
      "fold 2-3 \taccuracy:\t 0.9838709677419355 \tloss:\t 0.09988337383727977  \tf1 score:\t 0.9679586173751569\n",
      "fold 2-4 \taccuracy:\t 0.9792626728110599 \tloss:\t 0.05665649313633712  \tf1 score:\t 0.895832635015023\n",
      "fold 2-5 \taccuracy:\t 0.9792626728110599 \tloss:\t 0.009983623603172363  \tf1 score:\t 0.8961814289136424\n",
      "fold 2-6 \taccuracy:\t 0.9792626728110599 \tloss:\t 0.003651756921028899  \tf1 score:\t 0.89638411710335\n",
      "fold 2-7 \taccuracy:\t 0.9769585253456221 \tloss:\t 0.002787482053973094  \tf1 score:\t 0.89517405922503\n",
      "fold 2-8 \taccuracy:\t 0.9746543778801844 \tloss:\t 0.002344884038173969  \tf1 score:\t 0.8937674164219267\n",
      "fold 2-9 \taccuracy:\t 0.9769585253456221 \tloss:\t 0.0020757747606498455  \tf1 score:\t 0.89517405922503\n",
      "fold 2-10 \taccuracy:\t 0.9769585253456221 \tloss:\t 0.0016941475191468523  \tf1 score:\t 0.8765961045963547\n",
      "fold 2-11 \taccuracy:\t 0.9792626728110599 \tloss:\t 0.0015760032776033141  \tf1 score:\t 0.9645389350494545\n",
      "fold 2-12 \taccuracy:\t 0.9769585253456221 \tloss:\t 0.0013276585615309268  \tf1 score:\t 0.96314223578973\n",
      "fold 2-13 \taccuracy:\t 0.9769585253456221 \tloss:\t 0.001261290754295738  \tf1 score:\t 0.96314223578973\n",
      "fold 2-14 \taccuracy:\t 0.9769585253456221 \tloss:\t 0.0011070634078872241  \tf1 score:\t 0.96314223578973\n",
      "fold 3-0 \taccuracy:\t 0.988479262672811 \tloss:\t 5.434922624507855  \tf1 score:\t 0.8793252518787789\n",
      "fold 3-1 \taccuracy:\t 0.9861751152073732 \tloss:\t 0.30085737882601726  \tf1 score:\t 0.7826936373390618\n",
      "fold 3-2 \taccuracy:\t 0.9930875576036866 \tloss:\t 0.10823280864929075  \tf1 score:\t 0.9927208471254045\n",
      "fold 3-3 \taccuracy:\t 0.988479262672811 \tloss:\t 0.08551449296173394  \tf1 score:\t 0.8794067376594689\n",
      "fold 3-4 \taccuracy:\t 0.9861751152073732 \tloss:\t 0.057544353378836115  \tf1 score:\t 0.6920919604073761\n",
      "fold 3-5 \taccuracy:\t 0.9746543778801844 \tloss:\t 0.05778418632648162  \tf1 score:\t 0.7499120487075969\n",
      "fold 3-6 \taccuracy:\t 0.9907834101382489 \tloss:\t 0.04162883161584843  \tf1 score:\t 0.8927650903312203\n",
      "fold 3-7 \taccuracy:\t 0.9930875576036866 \tloss:\t 0.006637203441577076  \tf1 score:\t 0.9927269463600356\n",
      "fold 3-8 \taccuracy:\t 0.9907834101382489 \tloss:\t 0.0018791159807778576  \tf1 score:\t 0.9912345097039592\n",
      "fold 3-9 \taccuracy:\t 0.9907834101382489 \tloss:\t 0.001774391403156395  \tf1 score:\t 0.9912345097039592\n",
      "fold 3-10 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.001590843224892167  \tf1 score:\t 0.9937886150131049\n",
      "fold 3-11 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.001458201211542391  \tf1 score:\t 0.9937886150131049\n",
      "fold 3-12 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.0014000010726420967  \tf1 score:\t 0.9937886150131049\n",
      "fold 3-13 \taccuracy:\t 0.9953917050691244 \tloss:\t 0.0012774124510217227  \tf1 score:\t 0.9937886150131049\n",
      "fold 3-14 \taccuracy:\t 0.9930875576036866 \tloss:\t 0.0011576790907178026  \tf1 score:\t 0.9927208471254045\n",
      "fold 4-0 \taccuracy:\t 0.9676674364896074 \tloss:\t 5.766434946964527  \tf1 score:\t 0.7391276123654038\n",
      "fold 4-1 \taccuracy:\t 0.9722863741339491 \tloss:\t 0.47248433631090125  \tf1 score:\t 0.8090486087448341\n",
      "fold 4-2 \taccuracy:\t 0.976905311778291 \tloss:\t 0.05017046570920401  \tf1 score:\t 0.7027588336842674\n",
      "fold 4-3 \taccuracy:\t 0.9792147806004619 \tloss:\t 0.0494866934185948  \tf1 score:\t 0.8835530566782474\n",
      "fold 4-4 \taccuracy:\t 0.9838337182448037 \tloss:\t 0.009482148894962534  \tf1 score:\t 0.8922179446132629\n",
      "fold 4-5 \taccuracy:\t 0.9792147806004619 \tloss:\t 0.006702239562432153  \tf1 score:\t 0.7940689353389341\n",
      "fold 4-6 \taccuracy:\t 0.9792147806004619 \tloss:\t 0.00546126180029989  \tf1 score:\t 0.7997159150524098\n",
      "fold 4-7 \taccuracy:\t 0.9792147806004619 \tloss:\t 0.004677154540770751  \tf1 score:\t 0.7997159150524098\n",
      "fold 4-8 \taccuracy:\t 0.9792147806004619 \tloss:\t 0.004295632147467398  \tf1 score:\t 0.7997159150524098\n",
      "fold 4-9 \taccuracy:\t 0.9815242494226328 \tloss:\t 0.004107841611169627  \tf1 score:\t 0.8912786007327427\n",
      "fold 4-10 \taccuracy:\t 0.9653579676674365 \tloss:\t 0.017796534660788142  \tf1 score:\t 0.7674920993509531\n",
      "fold 4-11 \taccuracy:\t 0.9468822170900693 \tloss:\t 0.11575339085237797  \tf1 score:\t 0.6134772796251838\n",
      "fold 4-12 \taccuracy:\t 0.9884526558891455 \tloss:\t 0.09065131036365905  \tf1 score:\t 0.9683762155684564\n",
      "fold 4-13 \taccuracy:\t 0.9815242494226328 \tloss:\t 0.009509482731181348  \tf1 score:\t 0.8010414740250276\n",
      "fold 4-14 \taccuracy:\t 0.9815242494226328 \tloss:\t 0.003320142398279288  \tf1 score:\t 0.8010414740250276\n",
      "fold 5-0 \taccuracy:\t 0.9792147806004619 \tloss:\t 5.851531494794221  \tf1 score:\t 0.6255600560275508\n",
      "fold 5-1 \taccuracy:\t 0.9884526558891455 \tloss:\t 0.5065174778072357  \tf1 score:\t 0.8117981594019557\n",
      "fold 5-2 \taccuracy:\t 0.9884526558891455 \tloss:\t 0.1333304503686875  \tf1 score:\t 0.8892742716384475\n",
      "fold 5-3 \taccuracy:\t 0.9953810623556582 \tloss:\t 0.0801103343918181  \tf1 score:\t 0.8965916758384835\n",
      "fold 5-4 \taccuracy:\t 0.9976905311778291 \tloss:\t 0.1917918516555801  \tf1 score:\t 0.9980409512008025\n",
      "fold 5-5 \taccuracy:\t 0.976905311778291 \tloss:\t 0.029385284905088314  \tf1 score:\t 0.8988594522307253\n",
      "fold 5-6 \taccuracy:\t 0.9953810623556582 \tloss:\t 0.01022899752827251  \tf1 score:\t 0.8965916758384835\n",
      "fold 5-7 \taccuracy:\t 0.9953810623556582 \tloss:\t 0.005142342179295093  \tf1 score:\t 0.8965916758384835\n",
      "fold 5-8 \taccuracy:\t 0.9953810623556582 \tloss:\t 0.004395740967805162  \tf1 score:\t 0.8965916758384835\n",
      "fold 5-9 \taccuracy:\t 0.9930715935334873 \tloss:\t 0.004019126546656229  \tf1 score:\t 0.8628893489763054\n",
      "fold 5-10 \taccuracy:\t 0.9930715935334873 \tloss:\t 0.003656040280977071  \tf1 score:\t 0.8628893489763054\n",
      "fold 5-11 \taccuracy:\t 0.9930715935334873 \tloss:\t 0.0033350751658997916  \tf1 score:\t 0.8639640925034184\n",
      "fold 5-12 \taccuracy:\t 0.9930715935334873 \tloss:\t 0.003106905540429078  \tf1 score:\t 0.8639640925034184\n",
      "fold 5-13 \taccuracy:\t 0.9930715935334873 \tloss:\t 0.002918161380108647  \tf1 score:\t 0.8639640925034184\n",
      "fold 5-14 \taccuracy:\t 0.9930715935334873 \tloss:\t 0.002637715993914518  \tf1 score:\t 0.8639640925034184\n"
     ]
    }
   ],
   "source": [
    "#GO_Net\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split as ts\n",
    "\n",
    "data_rm_sparse=data_rm_sparse\n",
    "\n",
    "data_rm_sparse.index=annotation[\"celltype\"].to_list()\n",
    "\n",
    "###############################################################\n",
    "gene_to_TF_transform_matrix=pickle.load(open(\"%s/gene_to_TF_transform_matrix\" %base_dir,\"rb\"))\n",
    "TF_mask=pickle.load(open(\"%s/TF_mask\" %base_dir,\"rb\"))\n",
    "GO_mask=pickle.load(open(\"%s/GO_mask\" %base_dir,\"rb\"))\n",
    "GO_TF_mask=pickle.load(open(\"%s/GO_TF_mask\" %base_dir,\"rb\"))\n",
    "###############################################################\n",
    "#data_annotation = pd.read_csv('data/macparland/GSE115469_CellClusterType.txt', sep=\"\\t\")\n",
    "#index_rename_dict = {key: value for key, value in zip(data_annotation['CellName'], data_annotation['CellType'])}\n",
    "#$data_rm_sparse=data_rm_sparse.rename(index=index_rename_dict)\n",
    "\n",
    "#normalize by row\n",
    "#data_rm_sparse = data_rm_sparse.apply(lambda row: row / np.linalg.norm(row), axis=1)\n",
    "\n",
    "#merge similar cell types\n",
    "#data_rm_sparse.index = data_rm_sparse.index.str.replace('Hepatocyte_\\d+', 'Hepatocyte', regex=True)\n",
    "#data_rm_sparse.index = data_rm_sparse.index.str.replace('gamma-delta_T_Cells_\\d+', 'gamma-delta_T_Cells', regex=True)\n",
    "\n",
    "\n",
    "#filter low count cells\n",
    "#data_rm_sparse = data_rm_sparse[data_rm_sparse.index != 'Hepatic_Stellate_Cells']\n",
    "\n",
    "\n",
    "#novel_cell_type = ['Plasma_Cells']\n",
    "\n",
    "#data_rm_sparse_novel = data_rm_sparse[data_rm_sparse.index.isin(novel_cell_type)]\n",
    "#data_rm_sparse_rest = data_rm_sparse[~data_rm_sparse.index.isin(novel_cell_type)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classes=[]\n",
    "for celltype in data_rm_sparse.index:\n",
    "    if celltype not in classes:\n",
    "        classes.append(celltype)\n",
    "#print(len(classes),classes)\n",
    "\n",
    "\n",
    "label_dict_revese={}\n",
    "label_dict={}\n",
    "for i,celltype in enumerate(classes):\n",
    "    label_dict[celltype]=i\n",
    "    label_dict_revese[i]=celltype\n",
    "label_dict\n",
    "################################################################\n",
    "\n",
    "\n",
    "\n",
    "def gen_mask(row,col,percent=0.5,num_zeros=None):\n",
    "    if num_zeros is None:\n",
    "        #Total number being masked is 0.5 by default\n",
    "        num_zeros=int((row*col)*percent)\n",
    "    \n",
    "    mask=np.hstack([np.zeros(num_zeros),np.ones(row*col-num_zeros)])\n",
    "    np.random.shuffle(mask)\n",
    "    return mask.reshape(row,col)\n",
    "\n",
    "class LinearFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    autograd function which masks it's weights by 'mask'.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Not that both forward and backword are @staticmethod\n",
    "\n",
    "    \n",
    "    #bias, mask is an optional argument\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias=None, mask=None):\n",
    "        if mask is not None:\n",
    "            #change weight to 0 where mask == 0\n",
    "\n",
    "            weight=weight*mask\n",
    " \n",
    "        output=input.mm(weight.t())\n",
    "\n",
    "        if bias is not None:\n",
    "            output+=bias.unsqueeze(0).expand_as(output)\n",
    "        \n",
    "        ctx.save_for_backward(input, weight, bias, mask)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    #This function has noly a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        input,weight,bias,mask = ctx.saved_tensors\n",
    "        grad_input=grad_weight=grad_bias=grad_mask=None\n",
    "        \n",
    "        #These meeds_input_grad checks are optional and there only to improve efficiency.\n",
    "        #If you want to make your code simpler, you can skip them. Returning gradients for\n",
    "        #inputs that don't require it is not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input=grad_output.mm(weight)\n",
    "        \n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight=grad_output.t().mm(input)\n",
    "            \n",
    "            if mask is not None:\n",
    "                \n",
    "                #change grad_weight to 0 where mask == 0\n",
    "                grad_weight=grad_weight*mask\n",
    "\n",
    "        \n",
    "        #if bias is not None and ctx.need_input_grad[2]:\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_bias=grad_output.sum(0).squeeze(0)\n",
    "        \n",
    "        return grad_input,grad_weight,grad_bias,grad_mask\n",
    "    \n",
    "\n",
    "       \n",
    "class CustomizedLinear(nn.Module):\n",
    "    def __init__(self,input_features,output_features, bias=None, mask=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        mask [numpy array]:\n",
    "            The shape is (n_input_fearues,n_output_features).\n",
    "            The elements are 0 or 1 which delcare un-connected or connected.\n",
    "            \n",
    "        bias [bool]:\n",
    "            flg of bias.\n",
    "        \"\"\"\n",
    "        super(CustomizedLinear,self).__init__()\n",
    "        self.input_features=input_features\n",
    "        self.out_features=output_features\n",
    "        \n",
    "        #nn.Parameter is a spetial kind of Tensor, that will get\n",
    "        #automatically registered as Module's parameter once it's assigned\n",
    "        #as an attribute\n",
    "        self.weight=nn.Parameter(torch.Tensor(self.out_features,self.input_features))\n",
    "        \n",
    "        if bias:\n",
    "\n",
    "            self.bias=nn.Parameter(torch.Tensor(self.out_features))\n",
    "        else:\n",
    "            #You should always register all possible parameters, but the\n",
    "            #optinal ones can be None if you want.\n",
    "            self.register_parameter(\"bias\",None)\n",
    "            \n",
    "        #Initialize the above parameters (weight and bias). Important!\n",
    "        self.init_params()\n",
    "        \n",
    "        #mask should be registered after weight and bias\n",
    "        if mask is not None:\n",
    "            mask=torch.tensor(mask,dtype=torch.float).t()\n",
    "            self.mask=nn.Parameter(mask,requires_grad=False)\n",
    "        else:\n",
    "            self.register_parameter(\"mask\",None)\n",
    "\n",
    "        \n",
    "    def init_params(self):\n",
    "        stdv=1./math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv,stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv,stdv)\n",
    "                \n",
    "    def forward(self,input):\n",
    "        #See the autograd section for explanation of what happens here.\n",
    "        \n",
    "        output=LinearFunction.apply(input,self.weight,self.bias,self.mask)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def extra_repr(self):\n",
    "        #(Optional) Set the extra informatioin about this module. You can test\n",
    "        #it by printing an object of this class.\n",
    "        return \"input_features={}, output_features={}, bias={}, mask={}\".format(\n",
    "            self.input_features, self.out_features,\n",
    "            self.bias is not None, self.mask is not None)\n",
    "        \n",
    "        \n",
    "class GO_Net(nn.Module):\n",
    "    def __init__(self,in_size,out_size,ratio=[0.006525,0,0]):\n",
    "        super(GO_Net,self).__init__()\n",
    "\n",
    "        self.gene_number=len(gene_expressed_cell_number.index.tolist())    #6033\n",
    "        self.TF_number=1209\n",
    "        self.GO_number=len(GO_list)\n",
    "        self.class_number=3\n",
    "\n",
    "        self.gene_to_TF_transform_matrix=torch.tensor(gene_to_TF_transform_matrix,dtype=torch.float32)\n",
    "    \n",
    "        \n",
    "        self.bn0=nn.BatchNorm1d(self.gene_number)\n",
    "        #self.fc1=CustomizedLinear(in_size,2290,mask=gen_mask(3443,2290,ratio[0]))  \n",
    "        #self.fc1=CustomizedLinear(in_size,1946,mask=gen_mask(2903,1946,ratio[0]))        \n",
    "        self.fc1=CustomizedLinear(in_size,self.GO_number,mask=GO_mask)    #GO_term\n",
    "        self.gene_to_GO_layer=CustomizedLinear(in_size,self.GO_number,mask=GO_mask)    #GO_term\n",
    "        #self.fc1=CustomizedLinear(in_size,2290,mask=np.ones((3443,2290)))\n",
    "    \n",
    "        self.bn1=nn.BatchNorm1d(self.GO_number)\n",
    "                \n",
    "        self.fc2=CustomizedLinear(self.GO_number,out_size,mask=gen_mask(self.GO_number,out_size,ratio[1]))\n",
    "        self.bn2=nn.BatchNorm1d(out_size)\n",
    "\n",
    "        self.gene_to_TF_layer=CustomizedLinear(self.gene_number,self.TF_number,mask=TF_mask)\n",
    "        self.TF_to_GO_layer=CustomizedLinear(self.TF_number,self.GO_number,mask=GO_TF_mask)\n",
    "        \n",
    "        self.fc3=CustomizedLinear(100,100,mask=gen_mask(100,100,ratio[1]))\n",
    "\n",
    "        self.fc4=CustomizedLinear(100,out_size,mask=gen_mask(100,out_size,ratio[1]))\n",
    "        \n",
    "        self.relu=nn.ReLU()\n",
    "        self.leaky_relu=nn.LeakyReLU()\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        for module in self.modules():\n",
    "            if isinstance(module,nn.Linear):\n",
    "                nn.init.uniform_(module.weight,a=0,b=1)\n",
    "            elif isinstance(module,(nn.BatchNorm1d,nn.GroupNorm)):\n",
    "                nn.init.constant_(module.weight,1)\n",
    "                nn.init.constant_(module.bias,0)\n",
    "\n",
    "                        \n",
    "    def forward(self,x):\n",
    "\n",
    "        #x=self.bn0(x)\n",
    "        TF_residul=torch.matmul(x,self.gene_to_TF_transform_matrix)\n",
    "\n",
    "        TF_derived_from_gene=self.gene_to_TF_layer(x)\n",
    "\n",
    "        TF_sum=TF_residul+TF_derived_from_gene\n",
    "        #TF_sum=TF_derived_from_gene\n",
    "\n",
    "        GO_derived_from_TF=self.TF_to_GO_layer(TF_sum)\n",
    "\n",
    "        GO_derived_from_gene=self.gene_to_GO_layer(x)\n",
    "\n",
    "        GO_sum=GO_derived_from_TF+GO_derived_from_gene\n",
    "\n",
    "        #x=self.bn0(x)\n",
    "        #x=self.fc1(x)\n",
    "        #x=self.bn1(x)\n",
    "        #x=self.relu(x)\n",
    "        #x=self.dropout(x)\n",
    "        GO_sum=self.leaky_relu(GO_sum)\n",
    "\n",
    "        #x=torch.tanh(x) \n",
    "        #print(161,self.fc1.weight)\n",
    "        x=self.fc2(GO_sum)\n",
    "        #x=self.bn2(x)\n",
    "        #x=self.relu(x)\n",
    "        #x=self.leaky_relu(x)\n",
    "        #x=self.fc3(x)\n",
    "        #x=self.leaky_relu(x)\n",
    "        #x=self.fc4(x)\n",
    " \n",
    "        return x,GO_sum,TF_derived_from_gene,GO_derived_from_TF\n",
    "\n",
    "\"\"\"\n",
    "class Reconstraction(nn.Module):\n",
    "    def __init__(self,in_size,out_size):\n",
    "        super(Reconstraction,self).__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_size, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(500, 1000),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(1000, out_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\"\"\"     \n",
    "\n",
    "\n",
    " \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.x[index]\n",
    "        label = self.y[index]\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "def accuracy_score(y_test,y_pred):\n",
    "    t=0\n",
    "    f=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==y_pred[i]:\n",
    "            t+=1\n",
    "        else:\n",
    "            f+=1\n",
    "    return(t/(t+f))\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def make_weights_for_balanced_classes(dataset, nclasses):\n",
    "    count = [0] * nclasses\n",
    "    for item in dataset:\n",
    "        count[item[1]] += 1\n",
    "    weight_per_class = [0.] * nclasses\n",
    "    N = float(sum(count))\n",
    "    for i in range(nclasses):\n",
    "        weight_per_class[i] = N/float(count[i])\n",
    "    weight = [0] * len(dataset)\n",
    "    for idx, val in enumerate(dataset):\n",
    "        weight[idx] = weight_per_class[val[1]]\n",
    "    return weight\n",
    "\n",
    "\n",
    "class CustomWeightedRandomSampler(WeightedRandomSampler):\n",
    "    \"\"\"WeightedRandomSampler except allows for more than 2^24 samples to be sampled\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __iter__(self):\n",
    "        rand_tensor = np.random.choice(range(0, len(self.weights)),\n",
    "                                       size=self.num_samples,\n",
    "                                       p=self.weights.numpy() / torch.sum(self.weights).numpy(),\n",
    "                                       replace=self.replacement)\n",
    "        rand_tensor = torch.from_numpy(rand_tensor)\n",
    "        return iter(rand_tensor.tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#training\n",
    "input_size = len(data_rm_sparse.columns)\n",
    "output_size = len(classes)\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "#reconstraction_optimizer = optim.Adam(reconstraction_model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#reconstraction_criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "data_train_x=data_rm_sparse\n",
    "data_train_y=data_rm_sparse.index\n",
    "\n",
    "\n",
    "#5-fold cross validation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "# Generate 5-fold cross-validation indices\n",
    "kf = KFold(n_splits=num_folds, shuffle=False)\n",
    "fold_indices = list(kf.split(data))\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for fold, (train_indices, test_indices) in enumerate(fold_indices, start=1):\n",
    "\n",
    "    #define model and optimizer\n",
    "    model = GO_Net(input_size, output_size,ratio=[0,0,0])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    # Subset data and annotation based on indices\n",
    "    x_train = data_train_x.iloc[train_indices].to_numpy()\n",
    "    y_train = data_train_y[train_indices,]\n",
    "    \n",
    "    x_test = data_train_x.iloc[test_indices].to_numpy()\n",
    "    y_test = data_train_y[test_indices,]\n",
    "\n",
    "    # Continue with your operations on data_train, anno_train, data_test, and anno_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #x_train,x_test,y_train,y_test = ts(data_train_x.to_numpy(),data_train_y.to_numpy(),test_size=0.2,random_state=1, shuffle=True)\n",
    "\n",
    "    #x_train=x_train[0:400]\n",
    "    #y_train=y_train[0:400]\n",
    "\n",
    "    #label_dict={25:0,26:1,27:2,33:3,34:4}\n",
    "    y_train_relabeled=[label_dict[label] for label in y_train]\n",
    "    y_test_relabeled=[label_dict[label] for label in y_test]\n",
    "\n",
    "\n",
    "    #train_size=20000\n",
    "\n",
    "    #x_train=x_train[0:train_size]\n",
    "    #y_train_relabeled=y_train_relabeled[0:train_size]\n",
    "\n",
    "    train_data=MyDataset(x_train,y_train_relabeled)\n",
    "\n",
    "\n",
    "\n",
    "    #for unbalanced data\n",
    "    \"\"\"\n",
    "    weights=make_weights_for_balanced_classes(train_data,len(classes))\n",
    "    weights = torch.DoubleTensor(weights)\n",
    "    sampler = CustomWeightedRandomSampler(weights, len(weights))        #sampler for imbalanced classes\n",
    "    \"\"\"\n",
    "\n",
    "    #train_loader=DataLoader(train_data, batch_size=60, sampler=sampler)\n",
    "    train_loader=DataLoader(train_data, batch_size=60, shuffle=True)\n",
    "\n",
    "    num_epochs=15\n",
    "    # 训练模型\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        reconstraction_running_loss = 0.0\n",
    "\n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "            inputs, labels = batch\n",
    "            #print(labels)\n",
    "            inputs=Variable(inputs).to(torch.float32)\n",
    "            labels=Variable(labels).to(torch.long)\n",
    "            # 将梯度缓存清零\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 前向传播、计算损失和反向传播\n",
    "            outputs,_,_,_ = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            #reconstraction_input=reconstraction_model(outputs)\n",
    "            #reconstraction_loss = reconstraction_criterion(reconstraction_input, inputs)\n",
    "\n",
    "            #reconstraction_optimizer.zero_grad()\n",
    "\n",
    "            #combined_loss=loss+reconstraction_loss\n",
    "            #combined_loss.backward()\n",
    "            loss.backward()\n",
    "        \n",
    "            optimizer.step()\n",
    "            #reconstraction_optimizer.step()\n",
    "\n",
    "\n",
    "            #reconstraction_running_loss += reconstraction_loss.item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 40 == 0:\n",
    "                pass\n",
    "                #print(i)\n",
    "                #print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
    "            \n",
    "            if i>400:\n",
    "                break\n",
    "\n",
    "        test_data=MyDataset(x_test,y_test_relabeled)\n",
    "        test_loader=DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "        result=[]\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            inputs, labels = batch\n",
    "            \n",
    "            inputs=Variable(inputs).to(torch.float32)\n",
    "\n",
    "            labels=Variable(labels).to(torch.long)\n",
    "            \n",
    "            outputs,_,_,_ = model(inputs)\n",
    "            pred = list(torch.max(outputs, 1)[1].numpy())\n",
    "            result.extend(pred)\n",
    "            #print(pred,labels)\n",
    "            if i>100:\n",
    "                break\n",
    "        accuracy = accuracy_score(y_test_relabeled[0:len(result)],result)\n",
    "        f1_score = calculate_multiclass_f1_score(y_test_relabeled[0:len(result)],result)\n",
    "                #########\n",
    "        print(\"fold %s-%s\" %(fold,epoch),\"\\taccuracy:\\t\",accuracy,\"\\tloss:\\t\",running_loss / len(train_loader),\" \\tf1 score:\\t\",f1_score )\n",
    "        \n",
    "        #save model\n",
    "        #pickle.dump(model,open(\"model/GO_heart.model\",\"wb\"))\n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
